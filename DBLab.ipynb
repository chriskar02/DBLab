{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea1f308-88c2-4bc0-858c-059c0cc74a37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314dacf4-a42e-4ff1-a74c-72cccf8fd093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a228252-405c-45e3-9b8e-44e3182fec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both Dataframe and RDD implementations are to use 4 spark executors\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query1 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f2413-e2fc-4fd4-8cc5-4b1b8b5e1030",
   "metadata": {},
   "source": [
    "#### Dataframe Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03e83166-6544-4c45-89df-2fbd6459f7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|    AgeGroup|Count|\n",
      "+------------+-----+\n",
      "|      Adults|72610|\n",
      "|Young Adults|23472|\n",
      "|    Children|10724|\n",
      "|     Seniors| 3099|\n",
      "+------------+-----+\n",
      "\n",
      "Execution Time (DataFrame API): 19.31 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load Crime Data\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter for \"AGGRAVATED ASSAULT\"\n",
    "assault_data = crime_data.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# Age Groups\n",
    "categorized = assault_data.withColumn(\n",
    "    \"AgeGroup\",\n",
    "    when(col(\"Vict Age\") < 18, \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Seniors\")\n",
    ")\n",
    "\n",
    "# Group and count\n",
    "result_df = categorized.groupBy(\"AgeGroup\").agg(count(\"*\").alias(\"Count\")).orderBy(col(\"Count\").desc())\n",
    "\n",
    "# Show results\n",
    "result_df.show()\n",
    "\n",
    "# Stop timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution Time (DataFrame API): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3508f-d96e-4b65-a1da-03ef6b08332f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RDD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0816e675-e0fc-416a-9d2a-48c3655e522f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adults: 72610\n",
      "Young Adults: 23472\n",
      "Children: 10724\n",
      "Seniors: 3099\n",
      "Execution Time (RDD API): 21.01 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load dataset as RDD\n",
    "crime_rdd = spark.sparkContext.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "# Extract header and filter it out\n",
    "header = crime_rdd.first()\n",
    "crime_rdd = crime_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Parse CSV rows\n",
    "\n",
    "\n",
    "def parse_csv(line):\n",
    "    return list(csv.reader([line]))[0]\n",
    "\n",
    "\n",
    "parsed_rdd = crime_rdd.map(parse_csv)\n",
    "\n",
    "# Filter for \"AGGRAVATED ASSAULT\"\n",
    "assault_rdd = parsed_rdd.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[9])\n",
    "\n",
    "# Age groups\n",
    "age_group_rdd = assault_rdd.map(lambda row: (\n",
    "    \"Children\" if int(row[11]) < 18 else\n",
    "    \"Young Adults\" if 18 <= int(row[11]) <= 24 else\n",
    "    \"Adults\" if 25 <= int(row[11]) <= 64 else\n",
    "    \"Seniors\"\n",
    "))\n",
    "\n",
    "# Group and count\n",
    "result_rdd = age_group_rdd.map(lambda group: (group, 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Show results\n",
    "for group, count in result_rdd.collect():\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# Stop timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution Time (RDD API): {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Stop spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8d73b-0ffb-4457-8dc1-7feb85eeb62e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d891e09-2dcb-4d20-a00a-073e76044437",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i) Data_Frame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af06644-66b7-49ff-bf89-822675ccdc17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, when, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b18b9f5-8a58-4295-bea9-d9cb26a0934d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|AREA NAME  |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "DataFrame API Execution Time: 65.09 seconds"
     ]
    }
   ],
   "source": [
    "# Aggregation logic\n",
    "aggregated = crime_data.groupBy(\n",
    "    expr(\"substring(`DATE OCC`, 7, 4)\").alias(\"YEAR\"),  # Extract year from DATE OCC\n",
    "    col(\"AREA NAME\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    sum(when(~col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), 1).otherwise(0)).alias(\"closed_cases\")  # Non-\"UNK\"/\"Invest Cont\" are closed\n",
    ").withColumn(\"closed_case_rate\", col(\"closed_cases\") / col(\"total_cases\"))\n",
    "\n",
    "# Define window specification for ranking within each year\n",
    "window_spec = Window.partitionBy(\"YEAR\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "# Assign rank and filter top 3 precincts per year\n",
    "ranked = aggregated.withColumn(\"ranking\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"ranking\") <= 3) \\\n",
    "    .orderBy(\"YEAR\", \"ranking\")\n",
    "\n",
    "# Measure the end time for the DataFrame API operations\n",
    "dataframe_api_end_time = time.time()\n",
    "\n",
    "# Count rows for showing all results\n",
    "row_count = ranked.count()\n",
    "\n",
    "# Show all rows in the output\n",
    "ranked.show(truncate=False, n=row_count)\n",
    "\n",
    "# Print DataFrame API execution time\n",
    "print(f\"DataFrame API Execution Time: {dataframe_api_end_time - dataframe_api_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdfdbf-f665-4f30-8b46-ba31433e42cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii) SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6123857-69d6-4398-81c8-834d1a443ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|Precinct   |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "SQL API Execution Time: 34.04 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "sql_api_start_time = time.time()\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query\n",
    "sql_query = \"\"\"\n",
    "    WITH Aggregated AS (\n",
    "        SELECT\n",
    "            SUBSTRING(`DATE OCC`, 7, 4) AS YEAR,\n",
    "            `AREA NAME` AS Precinct,\n",
    "            COUNT(*) AS total_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) / COUNT(*) AS closed_case_rate\n",
    "        FROM crime_data\n",
    "        GROUP BY SUBSTRING(`DATE OCC`, 7, 4), `AREA NAME`\n",
    "    ),\n",
    "    Ranked AS (\n",
    "        SELECT\n",
    "            YEAR,\n",
    "            Precinct,\n",
    "            total_cases,\n",
    "            closed_cases,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM Aggregated\n",
    "    )\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        Precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM Ranked\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY YEAR, ranking\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(sql_query)\n",
    "\n",
    "# Count rows to ensure all results are shown\n",
    "row_count = result.count()\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False, n=row_count)\n",
    "\n",
    "# End timer\n",
    "sql_api_end_time = time.time()\n",
    "\n",
    "# Print SQL API execution time\n",
    "print(f\"SQL API Execution Time: {sql_api_end_time - sql_api_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec51df-f14f-4d29-8f04-ebd1f9d6c5f6",
   "metadata": {},
   "source": [
    "csv to parquet transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcc1766b-8647-4cc2-8c0a-da1763410475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save Crime Data as Parquet\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2803ec8-24d3-47ff-b332-6e986dbcdf4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to s3://groups-bucket-dblab-905418150721/group28/query2/ in Parquet format."
     ]
    }
   ],
   "source": [
    "# Save as a single Parquet file to the specified S3 bucket\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group28/query2/\"\n",
    "crime_data.repartition(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path} in Parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ea579d8-50e2-40cf-8534-62e1ebe6d07d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|Precinct   |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "SQL API Execution Time: 15.60 seconds"
     ]
    }
   ],
   "source": [
    "#parquet solution\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "sql_api_start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crime Dataset in Parquet format\n",
    "crime_data = spark.read.parquet(\n",
    "    \"s3://groups-bucket-dblab-905418150721/group28/query2/part-00000-fea3c04b-7961-41ea-8e05-d62534cf766e-c000.snappy.parquet\"\n",
    ")\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query\n",
    "sql_query = \"\"\"\n",
    "    WITH Aggregated AS (\n",
    "        SELECT\n",
    "            SUBSTRING(`DATE OCC`, 7, 4) AS YEAR,\n",
    "            `AREA NAME` AS Precinct,\n",
    "            COUNT(*) AS total_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) / COUNT(*) AS closed_case_rate\n",
    "        FROM crime_data\n",
    "        GROUP BY SUBSTRING(`DATE OCC`, 7, 4), `AREA NAME`\n",
    "    ),\n",
    "    Ranked AS (\n",
    "        SELECT\n",
    "            YEAR,\n",
    "            Precinct,\n",
    "            total_cases,\n",
    "            closed_cases,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM Aggregated\n",
    "    )\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        Precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM Ranked\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY YEAR, ranking\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(sql_query)\n",
    "\n",
    "# Count rows to ensure all results are shown\n",
    "row_count = result.count()\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False, n=row_count)\n",
    "\n",
    "# End timer\n",
    "sql_api_end_time = time.time()\n",
    "\n",
    "# Print SQL API execution time\n",
    "print(f\"SQL API Execution Time: {sql_api_end_time - sql_api_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6016180-3e1f-4ecf-aeb6-1cfdfafa4433",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061d9333-acd0-4e91-b16c-7df1f9b3415a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3840</td><td>application_1732639283265_3780</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3780/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3780_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Crime Per Person DESC NULLS LAST], true\n",
      "+- Project [COMM#72, Total Population#208L, Crime Count#217L, Crime Per Person#249, Median Income#197, Income per Person#256]\n",
      "   +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, Crime Per Person#249, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "      +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249]\n",
      "         +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197]\n",
      "            +- Join Inner, (COMM#72 = COMM#241)\n",
      "               :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "               :  +- Join LeftOuter, (COMM#72 = COMM#227)\n",
      "               :     :- Aggregate [COMM#72], [COMM#72, sum(Population#70L) AS Total Population#208L, sum(Housing#71L) AS Total Housing#210L]\n",
      "               :     :  +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :     :     +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, geojson_to_wkt(Coordinates#73)#85 AS wkt_geometry#86]\n",
      "               :     :        +- Project [properties#63.ZCTA10 AS Zip Code#69, properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72, geometry#62.coordinates AS Coordinates#73]\n",
      "               :     :           +- Relation [_corrupt_record#61,geometry#62,properties#63,type#64] json\n",
      "               :     +- Aggregate [COMM#227], [COMM#227, count(DR_NO#119) AS Crime Count#217L]\n",
      "               :        +- Project [COMM#227, DR_NO#119]\n",
      "               :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "               :              :- SubqueryAlias cr\n",
      "               :              :  +- SubqueryAlias crime\n",
      "               :              :     +- View (`crime`, [DR_NO#119,LAT#145,LON#146,geometry#179])\n",
      "               :              :        +- Project [DR_NO#119, LAT#145, LON#146,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "               :              :           +- Project [DR_NO#119, LAT#145, LON#146]\n",
      "               :              :              +- Filter (substring(DATE OCC#121, 7, 4) = 2010)\n",
      "               :              :                 +- Relation [DR_NO#119,Date Rptd#120,DATE OCC#121,TIME OCC#122,AREA #123,AREA NAME#124,Rpt Dist No#125,Part 1-2#126,Crm Cd#127,Crm Cd Desc#128,Mocodes#129,Vict Age#130,Vict Sex#131,Vict Descent#132,Premis Cd#133,Premis Desc#134,Weapon Used Cd#135,Weapon Desc#136,Status#137,Status Desc#138,Crm Cd 1#139,Crm Cd 2#140,Crm Cd 3#141,Crm Cd 4#142,... 4 more fields] csv\n",
      "               :              +- SubqueryAlias c\n",
      "               :                 +- SubqueryAlias census\n",
      "               :                    +- View (`census`, [Zip Code#224,Population#225L,Housing#226L,COMM#227,Coordinates#228,wkt_geometry#86,geometry#93])\n",
      "               :                       +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :                          +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, geojson_to_wkt(Coordinates#228)#85 AS wkt_geometry#86]\n",
      "               :                             +- Project [properties#222.ZCTA10 AS Zip Code#224, properties#222.POP_2010 AS Population#225L, properties#222.HOUSING10 AS Housing#226L, properties#222.COMM AS COMM#227, geometry#221.coordinates AS Coordinates#228]\n",
      "               :                                +- Relation [_corrupt_record#220,geometry#221,properties#222,type#223] json\n",
      "               +- Aggregate [COMM#241], [COMM#241, round(avg(Estimated Median Income#48), 2) AS Median Income#197]\n",
      "                  +- Project [Zip Code#42, Estimated Median Income#48, COMM#241]\n",
      "                     +- Join Inner, (Zip Code#42 = cast(Zip Code#238 as int))\n",
      "                        :- Project [Zip Code#42, Estimated Median Income#48]\n",
      "                        :  +- Project [Zip Code#42, Community#43, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "                        :     +- Relation [Zip Code#42,Community#43,Estimated Median Income#44] csv\n",
      "                        +- Project [Zip Code#238, COMM#241]\n",
      "                           +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "                              +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, geojson_to_wkt(Coordinates#242)#85 AS wkt_geometry#86]\n",
      "                                 +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.POP_2010 AS Population#239L, properties#236.HOUSING10 AS Housing#240L, properties#236.COMM AS COMM#241, geometry#235.coordinates AS Coordinates#242]\n",
      "                                    +- Relation [_corrupt_record#234,geometry#235,properties#236,type#237] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Total Population: bigint, Crime Count: bigint, Crime Per Person: double, Median Income: double, Income per Person: double\n",
      "Sort [Crime Per Person#249 DESC NULLS LAST], true\n",
      "+- Project [COMM#72, Total Population#208L, Crime Count#217L, Crime Per Person#249, Median Income#197, Income per Person#256]\n",
      "   +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, Crime Per Person#249, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "      +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249]\n",
      "         +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197]\n",
      "            +- Join Inner, (COMM#72 = COMM#241)\n",
      "               :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "               :  +- Join LeftOuter, (COMM#72 = COMM#227)\n",
      "               :     :- Aggregate [COMM#72], [COMM#72, sum(Population#70L) AS Total Population#208L, sum(Housing#71L) AS Total Housing#210L]\n",
      "               :     :  +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :     :     +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, geojson_to_wkt(Coordinates#73)#85 AS wkt_geometry#86]\n",
      "               :     :        +- Project [properties#63.ZCTA10 AS Zip Code#69, properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72, geometry#62.coordinates AS Coordinates#73]\n",
      "               :     :           +- Relation [_corrupt_record#61,geometry#62,properties#63,type#64] json\n",
      "               :     +- Aggregate [COMM#227], [COMM#227, count(DR_NO#119) AS Crime Count#217L]\n",
      "               :        +- Project [COMM#227, DR_NO#119]\n",
      "               :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "               :              :- SubqueryAlias cr\n",
      "               :              :  +- SubqueryAlias crime\n",
      "               :              :     +- View (`crime`, [DR_NO#119,LAT#145,LON#146,geometry#179])\n",
      "               :              :        +- Project [DR_NO#119, LAT#145, LON#146,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "               :              :           +- Project [DR_NO#119, LAT#145, LON#146]\n",
      "               :              :              +- Filter (substring(DATE OCC#121, 7, 4) = 2010)\n",
      "               :              :                 +- Relation [DR_NO#119,Date Rptd#120,DATE OCC#121,TIME OCC#122,AREA #123,AREA NAME#124,Rpt Dist No#125,Part 1-2#126,Crm Cd#127,Crm Cd Desc#128,Mocodes#129,Vict Age#130,Vict Sex#131,Vict Descent#132,Premis Cd#133,Premis Desc#134,Weapon Used Cd#135,Weapon Desc#136,Status#137,Status Desc#138,Crm Cd 1#139,Crm Cd 2#140,Crm Cd 3#141,Crm Cd 4#142,... 4 more fields] csv\n",
      "               :              +- SubqueryAlias c\n",
      "               :                 +- SubqueryAlias census\n",
      "               :                    +- View (`census`, [Zip Code#224,Population#225L,Housing#226L,COMM#227,Coordinates#228,wkt_geometry#86,geometry#93])\n",
      "               :                       +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :                          +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, geojson_to_wkt(Coordinates#228)#85 AS wkt_geometry#86]\n",
      "               :                             +- Project [properties#222.ZCTA10 AS Zip Code#224, properties#222.POP_2010 AS Population#225L, properties#222.HOUSING10 AS Housing#226L, properties#222.COMM AS COMM#227, geometry#221.coordinates AS Coordinates#228]\n",
      "               :                                +- Relation [_corrupt_record#220,geometry#221,properties#222,type#223] json\n",
      "               +- Aggregate [COMM#241], [COMM#241, round(avg(Estimated Median Income#48), 2) AS Median Income#197]\n",
      "                  +- Project [Zip Code#42, Estimated Median Income#48, COMM#241]\n",
      "                     +- Join Inner, (Zip Code#42 = cast(Zip Code#238 as int))\n",
      "                        :- Project [Zip Code#42, Estimated Median Income#48]\n",
      "                        :  +- Project [Zip Code#42, Community#43, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "                        :     +- Relation [Zip Code#42,Community#43,Estimated Median Income#44] csv\n",
      "                        +- Project [Zip Code#238, COMM#241]\n",
      "                           +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "                              +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, geojson_to_wkt(Coordinates#242)#85 AS wkt_geometry#86]\n",
      "                                 +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.POP_2010 AS Population#239L, properties#236.HOUSING10 AS Housing#240L, properties#236.COMM AS COMM#241, geometry#235.coordinates AS Coordinates#242]\n",
      "                                    +- Relation [_corrupt_record#234,geometry#235,properties#236,type#237] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Crime Per Person#249 DESC NULLS LAST], true\n",
      "+- Project [COMM#72, Total Population#208L, Crime Count#217L, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249, Median Income#197, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "   +- Join Inner, (COMM#72 = COMM#241)\n",
      "      :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "      :  +- Join LeftOuter, (COMM#72 = COMM#227)\n",
      "      :     :- Aggregate [COMM#72], [COMM#72, sum(Population#70L) AS Total Population#208L, sum(Housing#71L) AS Total Housing#210L]\n",
      "      :     :  +- Project [properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72]\n",
      "      :     :     +- Filter isnotnull(properties#63.COMM)\n",
      "      :     :        +- Relation [_corrupt_record#61,geometry#62,properties#63,type#64] json\n",
      "      :     +- Aggregate [COMM#227], [COMM#227, count(DR_NO#119) AS Crime Count#217L]\n",
      "      :        +- Project [COMM#227, DR_NO#119]\n",
      "      :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "      :              :- Project [DR_NO#119,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "      :              :  +- Filter ((isnotnull(DATE OCC#121) AND (substring(DATE OCC#121, 7, 4) = 2010)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :              :     +- Relation [DR_NO#119,Date Rptd#120,DATE OCC#121,TIME OCC#122,AREA #123,AREA NAME#124,Rpt Dist No#125,Part 1-2#126,Crm Cd#127,Crm Cd Desc#128,Mocodes#129,Vict Age#130,Vict Sex#131,Vict Descent#132,Premis Cd#133,Premis Desc#134,Weapon Used Cd#135,Weapon Desc#136,Status#137,Status Desc#138,Crm Cd 1#139,Crm Cd 2#140,Crm Cd 3#141,Crm Cd 4#142,... 4 more fields] csv\n",
      "      :              +- Project [properties#222.COMM AS COMM#227,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "      :                 +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#271]\n",
      "      :                    +- Project [geometry#221, properties#222]\n",
      "      :                       +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**  )\n",
      "      :                          +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#270]\n",
      "      :                             +- Project [geometry#221, properties#222]\n",
      "      :                                +- Filter isnotnull(properties#222.COMM)\n",
      "      :                                   +- Relation [_corrupt_record#220,geometry#221,properties#222,type#223] json\n",
      "      +- Aggregate [COMM#241], [COMM#241, round(avg(Estimated Median Income#48), 2) AS Median Income#197]\n",
      "         +- Project [Estimated Median Income#48, COMM#241]\n",
      "            +- Join Inner, (Zip Code#42 = cast(Zip Code#238 as int))\n",
      "               :- Project [Zip Code#42, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "               :  +- Filter isnotnull(Zip Code#42)\n",
      "               :     +- Relation [Zip Code#42,Community#43,Estimated Median Income#44] csv\n",
      "               +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.COMM AS COMM#241]\n",
      "                  +- Filter (isnotnull(properties#236.ZCTA10) AND isnotnull(properties#236.COMM))\n",
      "                     +- Relation [_corrupt_record#234,geometry#235,properties#236,type#237] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Crime Per Person#249 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Crime Per Person#249 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=251]\n",
      "      +- Project [COMM#72, Total Population#208L, Crime Count#217L, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249, Median Income#197, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "         +- SortMergeJoin [COMM#72], [COMM#241], Inner\n",
      "            :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "            :  +- SortMergeJoin [COMM#72], [COMM#227], LeftOuter\n",
      "            :     :- Sort [COMM#72 ASC NULLS FIRST], false, 0\n",
      "            :     :  +- HashAggregate(keys=[COMM#72], functions=[sum(Population#70L), sum(Housing#71L)], output=[COMM#72, Total Population#208L, Total Housing#210L], schema specialized)\n",
      "            :     :     +- Exchange hashpartitioning(COMM#72, 1000), ENSURE_REQUIREMENTS, [plan_id=228]\n",
      "            :     :        +- HashAggregate(keys=[COMM#72], functions=[partial_sum(Population#70L), partial_sum(Housing#71L)], output=[COMM#72, sum#273L, sum#275L], schema specialized)\n",
      "            :     :           +- Project [properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72]\n",
      "            :     :              +- Filter isnotnull(properties#63.COMM)\n",
      "            :     :                 +- FileScan json [properties#63] Batched: false, DataFilters: [isnotnull(properties#63.COMM)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CI...\n",
      "            :     +- Sort [COMM#227 ASC NULLS FIRST], false, 0\n",
      "            :        +- HashAggregate(keys=[COMM#227], functions=[count(DR_NO#119)], output=[COMM#227, Crime Count#217L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#227, 1000), ENSURE_REQUIREMENTS, [plan_id=230]\n",
      "            :              +- HashAggregate(keys=[COMM#227], functions=[partial_count(DR_NO#119)], output=[COMM#227, count#277L], schema specialized)\n",
      "            :                 +- Project [COMM#227, DR_NO#119]\n",
      "            :                    +- RangeJoin geometry#179: geometry, geometry#93: geometry, WITHIN\n",
      "            :                       :- Project [DR_NO#119,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "            :                       :  +- Filter ((isnotnull(DATE OCC#121) AND (substring(DATE OCC#121, 7, 4) = 2010)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "            :                       :     +- FileScan csv [DR_NO#119,DATE OCC#121,LAT#145,LON#146] Batched: false, DataFilters: [isnotnull(DATE OCC#121), (substring(DATE OCC#121, 7, 4) = 2010), isnotnull( **org.apache.spark.s..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(DATE OCC)], ReadSchema: struct<DR_NO:int,DATE OCC:string,LAT:double,LON:double>\n",
      "            :                       +- Project [properties#222.COMM AS COMM#227,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "            :                          +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#271]\n",
      "            :                             +- Project [geometry#221, properties#222]\n",
      "            :                                +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**  )\n",
      "            :                                   +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#270]\n",
      "            :                                      +- Filter isnotnull(properties#222.COMM)\n",
      "            :                                         +- FileScan json [geometry#221,properties#222] Batched: false, DataFilters: [isnotnull(properties#222.COMM)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>,properties:struct<BG1...\n",
      "            +- Sort [COMM#241 ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[COMM#241], functions=[avg(Estimated Median Income#48)], output=[COMM#241, Median Income#197], schema specialized)\n",
      "                  +- Exchange hashpartitioning(COMM#241, 1000), ENSURE_REQUIREMENTS, [plan_id=243]\n",
      "                     +- HashAggregate(keys=[COMM#241], functions=[partial_avg(Estimated Median Income#48)], output=[COMM#241, sum#280, count#281L], schema specialized)\n",
      "                        +- Project [Estimated Median Income#48, COMM#241]\n",
      "                           +- BroadcastHashJoin [Zip Code#42], [cast(Zip Code#238 as int)], Inner, BuildLeft, false\n",
      "                              :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=238]\n",
      "                              :  +- Project [Zip Code#42, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "                              :     +- Filter isnotnull(Zip Code#42)\n",
      "                              :        +- FileScan csv [Zip Code#42,Estimated Median Income#44] Batched: false, DataFilters: [isnotnull(Zip Code#42)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "                              +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.COMM AS COMM#241]\n",
      "                                 +- Filter (isnotnull(properties#236.ZCTA10) AND isnotnull(properties#236.COMM))\n",
      "                                    +- FileScan json [properties#236] Batched: false, DataFilters: [isnotnull(properties#236.ZCTA10), isnotnull(properties#236.COMM)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CI...\n",
      "\n",
      "+------------------+----------------+-----------+----------------+-------------+-----------------+\n",
      "|              Area|Total Population|Crime Count|Crime Per Person|Median Income|Income per Person|\n",
      "+------------------+----------------+-----------+----------------+-------------+-----------------+\n",
      "|            Vernon|             112|         71|        0.633929|     18331.28|          4746.49|\n",
      "|   West Chatsworth|              12|          3|            0.25|      61339.0|         20446.33|\n",
      "|           West LA|             702|        154|        0.219373|     87689.58|          8119.41|\n",
      "|          Downtown|           23647|       4192|        0.177274|     27621.85|         19977.86|\n",
      "|      Little Tokyo|            3386|        465|         0.13733|     23004.07|         14110.88|\n",
      "|Wholesale District|           37156|       4105|         0.11048|     23835.11|          7286.02|\n",
      "|      Leimert Park|           14859|       1475|        0.099266|     36948.59|          16085.9|\n",
      "| Faircrest Heights|            3443|        327|        0.094975|     52901.34|          20834.8|\n",
      "|   Sycamore Square|             635|         60|        0.094488|     61147.88|         30814.68|\n",
      "|         Hollywood|           62412|       5681|        0.091024|      45290.0|         25948.94|\n",
      "|     Vermont Vista|           37550|       3364|        0.089587|     30258.84|          8460.39|\n",
      "|        Exposition|            3238|        287|        0.088635|      38330.0|         12322.89|\n",
      "|Century Palms/Cove|           30692|       2703|        0.088069|     32851.29|          8572.46|\n",
      "|     Baldwin Hills|           28637|       2450|        0.085554|     37297.92|         17391.46|\n",
      "|            Venice|           32625|       2652|        0.081287|     81028.74|         46575.69|\n",
      "|       Rancho Park|            6295|        511|        0.081176|      87283.0|         38740.06|\n",
      "|         Hyde Park|           27943|       2241|        0.080199|     38196.31|         14143.69|\n",
      "| Manchester Square|            8247|        654|        0.079302|      39269.0|         14589.57|\n",
      "|    Vermont Knolls|           16678|       1303|        0.078127|     31295.94|         10440.74|\n",
      "|      Harvard Park|           36447|       2832|        0.077702|     33372.25|          9909.03|\n",
      "+------------------+----------------+-----------+----------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time: 53.19 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, regexp_replace, round, avg, expr, udf, count, sum as spark_sum\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "# Initialize Spark session with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query3_LA_Analysis\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register Sedona\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and preprocess income dataset\n",
    "income_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Load and preprocess census dataset\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "# Convert GeoJSON to WKT for Sedona\n",
    "\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "census_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "census_df = census_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "# Load and preprocess crime dataset\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2010\")\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Perform spatial join using Sedona\n",
    "census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "crime_with_comm_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "# Precompute income by COMM to avoid recomputation\n",
    "income_by_comm = income_df.select(\n",
    "    col(\"Zip Code\"),\n",
    "    col(\"Estimated Median Income\")\n",
    ").join(\n",
    "    census_df.select(\"Zip Code\", \"COMM\"),\n",
    "    on=\"Zip Code\",\n",
    "    how=\"inner\"\n",
    ").groupBy(\"COMM\").agg(\n",
    "    round(avg(\"Estimated Median Income\"), 2).alias(\"Median Income\")\n",
    ")\n",
    "\n",
    "# Calculate the final DataFrame with all required information\n",
    "final_df = census_df.groupBy(\"COMM\").agg(\n",
    "    spark_sum(\"Population\").alias(\"Total Population\"),\n",
    "    spark_sum(\"Housing\").alias(\"Total Housing\")\n",
    ").join(\n",
    "    crime_with_comm_df.groupBy(\"COMM\").agg(\n",
    "        count(\"DR_NO\").alias(\"Crime Count\")\n",
    "    ),\n",
    "    on=\"COMM\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    income_by_comm,\n",
    "    on=\"COMM\",\n",
    "    how=\"inner\"\n",
    ").withColumn(\n",
    "    \"Crime Per Person\",\n",
    "    round(col(\"Crime Count\") / col(\"Total Population\"), 6)\n",
    ").withColumn(\n",
    "    \"Income per Person\",\n",
    "    round((col(\"Median Income\") * col(\"Total Housing\")) / col(\"Total Population\"), 2)\n",
    ").select(\n",
    "    col(\"COMM\"),\n",
    "    col(\"Total Population\"),\n",
    "    col(\"Crime Count\"),\n",
    "    col(\"Crime Per Person\"),\n",
    "    col(\"Median Income\"),\n",
    "    col(\"Income per Person\")\n",
    ").orderBy(col(\"Crime Per Person\").desc())\n",
    "\n",
    "final_df.explain(mode=\"extended\")\n",
    "\n",
    "# Save results for query 4\n",
    "query3_df = final_df.select(\n",
    "    col(\"COMM\"),\n",
    "    col(\"Income per Person\")\n",
    ")\n",
    "\n",
    "query3_df.write.mode(\"ignore\").option(\"header\", \"true\").csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/\")\n",
    "\n",
    "# Display final results\n",
    "final_df_area = final_df.withColumnRenamed(\"COMM\", \"Area\")\n",
    "final_df_area.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c664ef8-441a-4418-af83-1fedac12ecb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de95ad-7d26-42bd-b082-277144d2c433",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1core/2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4b1db4-5815-4885-ae58-321c699dacb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victim Counts by Race in Top 3 High-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|               White|      544|\n",
      "|               Other|       73|\n",
      "|Hispanic/Latin/Me...|       60|\n",
      "|             Unknown|       41|\n",
      "|               Black|       37|\n",
      "|         Other Asian|       15|\n",
      "|             Chinese|        1|\n",
      "|American Indian/A...|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "\n",
      "Victim Counts by Race in Bottom 3 Low-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|Hispanic/Latin/Me...|     1494|\n",
      "|               Black|      456|\n",
      "|               Other|       53|\n",
      "|               White|       29|\n",
      "|         Other Asian|        4|\n",
      "|             Unknown|        3|\n",
      "|            Filipino|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "Total execution time: 40.14 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, to_date, year, regexp_replace, count, expr, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "# Create Spark Session with Sedona enabled\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spatial Join - 1 Core, 2GB Memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", 1) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register Sedona\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load census.geojson\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "# Convert to wkt with udf\n",
    "\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "\n",
    "wkt_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "wkt_df.select(\"wkt_geometry\")\n",
    "wkt_df = wkt_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "wkt_census_df = wkt_df.select(\"COMM\", \"geometry\")\n",
    "\n",
    "# Load crime.csv\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    "\n",
    "# Select necessary columns and create point geometries\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Load income per person from query3\n",
    "income_per_person_df = spark.read.csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/part-00000-5e1158bd-fb96-45fd-a2a9-7bf304269966-c000.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load race.csv\n",
    "race_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform Spatial Join\n",
    "# Register spatial DataFrames as tables\n",
    "wkt_census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "# Perform spatial join using Sedona's ST_Contains\n",
    "crime_with_zip_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO, cr.`Vict Descent`\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "# Aggregate Crimes by Zip Code and Race\n",
    "crime_with_race_df = crime_with_zip_df.join(race_df, \"Vict Descent\", \"inner\")\n",
    "grouped_df = crime_with_race_df.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"*\").alias(\"CrimeCount\"))\n",
    "\n",
    "# Filter Top and Bottom 3 Communities by Income\n",
    "top_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").desc()).select(\"COMM\").limit(3)\n",
    "bottom_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").asc()).select(\"COMM\").limit(3)\n",
    "\n",
    "# Convert top and bottom communities to lists for filtering\n",
    "top_3_comm_list = [row[\"COMM\"] for row in top_3_comm.collect()]\n",
    "bottom_3_comm_list = [row[\"COMM\"] for row in bottom_3_comm.collect()]\n",
    "\n",
    "# Filter crime data for Top and Bottom Communities\n",
    "filtered_crime_top = crime_with_race_df.filter(col(\"COMM\").isin(top_3_comm_list))\n",
    "filtered_crime_bottom = crime_with_race_df.filter(col(\"COMM\").isin(bottom_3_comm_list))\n",
    "\n",
    "\n",
    "# Aggregate Crime Counts by Race for Each Group\n",
    "top_crime_race_df = filtered_crime_top.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "bottom_crime_race_df = filtered_crime_bottom.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "# Display Results\n",
    "print(\"Victim Counts by Race in Top 3 High-Income Communities:\")\n",
    "top_crime_race_df.show()\n",
    "\n",
    "print(\"\\nVictim Counts by Race in Bottom 3 Low-Income Communities:\")\n",
    "bottom_crime_race_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf0bd5-d3b1-422c-9bec-69d130f91620",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2cores/4GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e098e2-d4e5-4c73-8cb6-d34ed845bde1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3672</td><td>application_1732639283265_3618</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3618/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3618_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victim Counts by Race in Top 3 High-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|               White|      544|\n",
      "|               Other|       73|\n",
      "|Hispanic/Latin/Me...|       60|\n",
      "|             Unknown|       41|\n",
      "|               Black|       37|\n",
      "|         Other Asian|       15|\n",
      "|             Chinese|        1|\n",
      "|American Indian/A...|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "\n",
      "Victim Counts by Race in Bottom 3 Low-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|Hispanic/Latin/Me...|     1494|\n",
      "|               Black|      456|\n",
      "|               Other|       53|\n",
      "|               White|       29|\n",
      "|         Other Asian|        4|\n",
      "|             Unknown|        3|\n",
      "|            Filipino|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "Total execution time: 34.99 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, to_date, year, regexp_replace, count, expr, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spatial Join - 1 Core, 2GB Memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "\n",
    "wkt_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "wkt_df.select(\"wkt_geometry\")\n",
    "wkt_df = wkt_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "wkt_census_df = wkt_df.select(\"COMM\", \"geometry\")\n",
    "\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    "\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "income_per_person_df = spark.read.csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/part-00000-5e1158bd-fb96-45fd-a2a9-7bf304269966-c000.csv\", header=True, inferSchema=True)\n",
    "\n",
    "race_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "wkt_census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "crime_with_zip_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO, cr.`Vict Descent`\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "crime_with_race_df = crime_with_zip_df.join(race_df, \"Vict Descent\", \"inner\")\n",
    "grouped_df = crime_with_race_df.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"*\").alias(\"CrimeCount\"))\n",
    "\n",
    "top_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").desc()).select(\"COMM\").limit(3)\n",
    "bottom_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").asc()).select(\"COMM\").limit(3)\n",
    "\n",
    "top_3_comm_list = [row[\"COMM\"] for row in top_3_comm.collect()]\n",
    "bottom_3_comm_list = [row[\"COMM\"] for row in bottom_3_comm.collect()]\n",
    "\n",
    "filtered_crime_top = crime_with_race_df.filter(col(\"COMM\").isin(top_3_comm_list))\n",
    "filtered_crime_bottom = crime_with_race_df.filter(col(\"COMM\").isin(bottom_3_comm_list))\n",
    "\n",
    "top_crime_race_df = filtered_crime_top.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "bottom_crime_race_df = filtered_crime_bottom.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "print(\"Victim Counts by Race in Top 3 High-Income Communities:\")\n",
    "top_crime_race_df.show()\n",
    "\n",
    "print(\"\\nVictim Counts by Race in Bottom 3 Low-Income Communities:\")\n",
    "bottom_crime_race_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f18d5-ad07-4f96-bbb3-e5a6b2036a99",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4cores/8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fbf6cf-bc53-487a-84b0-711498a71efe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3674</td><td>application_1732639283265_3620</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3620/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3620_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victim Counts by Race in Top 3 High-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|               White|      544|\n",
      "|               Other|       73|\n",
      "|Hispanic/Latin/Me...|       60|\n",
      "|             Unknown|       41|\n",
      "|               Black|       37|\n",
      "|         Other Asian|       15|\n",
      "|             Chinese|        1|\n",
      "|American Indian/A...|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "\n",
      "Victim Counts by Race in Bottom 3 Low-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|Hispanic/Latin/Me...|     1494|\n",
      "|               Black|      456|\n",
      "|               Other|       53|\n",
      "|               White|       29|\n",
      "|         Other Asian|        4|\n",
      "|             Unknown|        3|\n",
      "|            Filipino|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "Total execution time: 29.24 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, to_date, year, regexp_replace, count, expr, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spatial Join - 1 Core, 2GB Memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", 4) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "\n",
    "wkt_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "wkt_df.select(\"wkt_geometry\")\n",
    "wkt_df = wkt_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "wkt_census_df = wkt_df.select(\"COMM\", \"geometry\")\n",
    "\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    "\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "income_per_person_df = spark.read.csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/part-00000-5e1158bd-fb96-45fd-a2a9-7bf304269966-c000.csv\", header=True, inferSchema=True)\n",
    "\n",
    "race_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "wkt_census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "crime_with_zip_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO, cr.`Vict Descent`\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "crime_with_race_df = crime_with_zip_df.join(race_df, \"Vict Descent\", \"inner\")\n",
    "grouped_df = crime_with_race_df.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"*\").alias(\"CrimeCount\"))\n",
    "\n",
    "top_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").desc()).select(\"COMM\").limit(3)\n",
    "bottom_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").asc()).select(\"COMM\").limit(3)\n",
    "\n",
    "top_3_comm_list = [row[\"COMM\"] for row in top_3_comm.collect()]\n",
    "bottom_3_comm_list = [row[\"COMM\"] for row in bottom_3_comm.collect()]\n",
    "\n",
    "filtered_crime_top = crime_with_race_df.filter(col(\"COMM\").isin(top_3_comm_list))\n",
    "filtered_crime_bottom = crime_with_race_df.filter(col(\"COMM\").isin(bottom_3_comm_list))\n",
    "\n",
    "top_crime_race_df = filtered_crime_top.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "bottom_crime_race_df = filtered_crime_bottom.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "print(\"Victim Counts by Race in Top 3 High-Income Communities:\")\n",
    "top_crime_race_df.show()\n",
    "\n",
    "print(\"\\nVictim Counts by Race in Bottom 3 Low-Income Communities:\")\n",
    "bottom_crime_race_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647757c-bcb4-43f3-8e25-d50d89bfe33c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce7840-4027-44aa-90c4-1ae72eac9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, when, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11819c4e-18f4-4f90-9d30-2dab142d1e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the CSV file:\n",
      "['X', 'Y', 'FID', 'DIVISION', 'LOCATION', 'PREC']\n",
      "Schema of the CSV file:\n",
      "root\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      " |-- FID: integer (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- PREC: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect columns\n",
    "print(\"Columns in the CSV file:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Inspect schema\n",
    "print(\"Schema of the CSV file:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aea1b0-e9b7-48c0-8d73-54240f97213c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2 executors × 4 cores/8GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff827f61-6fda-4e8e-8aa2-b134061a0acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.02043779072548565 |\n",
      "|VAN NUYS        |211457          |0.028653154590629136|\n",
      "|WILSHIRE        |198150          |0.026312166557481587|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.01729162112331338 |\n",
      "|NORTH HOLLYWOOD |171159          |0.026115214222567722|\n",
      "|77TH STREET     |167323          |0.016584871496068188|\n",
      "|PACIFIC         |157468          |0.037495777088312074|\n",
      "|CENTRAL         |154474          |0.009868086849235298|\n",
      "|SOUTHEAST       |151999          |0.024150127195506455|\n",
      "|RAMPART         |149675          |0.014730484635455721|\n",
      "|TOPANGA         |147167          |0.03243890335156792 |\n",
      "|WEST VALLEY     |130933          |0.02897360719640746 |\n",
      "|HARBOR          |130206          |3.2997622866934675  |\n",
      "|FOOTHILL        |122515          |0.04125740608010438 |\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859795|\n",
      "|HOLLENBECK      |119329          |0.026407445235885316|\n",
      "|NEWTON          |109078          |0.015890866822603905|\n",
      "|MISSION         |109009          |0.035032007153604966|\n",
      "|NORTHEAST       |105687          |0.03907902069344001 |\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 56.37 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-sql-3.0_2.12:1.6.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e597d39-633e-4996-ba23-69e99542d912",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4 executors × 2 cores/4GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "901e1710-1a32-4790-bade-85a902d818e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.020437790725485655|\n",
      "|VAN NUYS        |211457          |0.02865315459062913 |\n",
      "|WILSHIRE        |198150          |0.026312166557481583|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.01729162112331337 |\n",
      "|NORTH HOLLYWOOD |171159          |0.026115214222567722|\n",
      "|77TH STREET     |167323          |0.016584871496068194|\n",
      "|PACIFIC         |157468          |0.03749577708831207 |\n",
      "|CENTRAL         |154474          |0.0098680868492353  |\n",
      "|SOUTHEAST       |151999          |0.024150127195506462|\n",
      "|RAMPART         |149675          |0.014730484635455718|\n",
      "|TOPANGA         |147167          |0.03243890335156792 |\n",
      "|WEST VALLEY     |130933          |0.028973607196407465|\n",
      "|HARBOR          |130206          |3.2997622866934675  |\n",
      "|FOOTHILL        |122515          |0.041257406080104385|\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859802|\n",
      "|HOLLENBECK      |119329          |0.02640744523588532 |\n",
      "|NEWTON          |109078          |0.0158908668226039  |\n",
      "|MISSION         |109009          |0.03503200715360496 |\n",
      "|NORTHEAST       |105687          |0.039079020693440006|\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 46.34 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with specified resources\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9eb7b-22fa-45fb-a9cf-c1f1aa0635b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 8 executors × 1 core/2 GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61735c16-b7d8-4e55-8d3c-7422fdc72acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.02043779072548566 |\n",
      "|VAN NUYS        |211457          |0.028653154590629126|\n",
      "|WILSHIRE        |198150          |0.026312166557481583|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.017291621123313373|\n",
      "|NORTH HOLLYWOOD |171159          |0.02611521422256772 |\n",
      "|77TH STREET     |167323          |0.016584871496068188|\n",
      "|PACIFIC         |157468          |0.037495777088312074|\n",
      "|CENTRAL         |154474          |0.009868086849235298|\n",
      "|SOUTHEAST       |151999          |0.02415012719550645 |\n",
      "|RAMPART         |149675          |0.014730484635455718|\n",
      "|TOPANGA         |147167          |0.03243890335156791 |\n",
      "|WEST VALLEY     |130933          |0.02897360719640747 |\n",
      "|HARBOR          |130206          |3.299762286693468   |\n",
      "|FOOTHILL        |122515          |0.041257406080104385|\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859795|\n",
      "|HOLLENBECK      |119329          |0.026407445235885323|\n",
      "|NEWTON          |109078          |0.0158908668226039  |\n",
      "|MISSION         |109009          |0.03503200715360496 |\n",
      "|NORTHEAST       |105687          |0.039079020693440006|\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 16.25 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with specified resources\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "27bd26d4-7c66-4cf7-99da-28cf5dbdb03d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preview of the Income DataFrame:\n",
      "+--------+-------------------------------------------------------------------------------------------------------+-----------------------+\n",
      "|Zip Code|Community                                                                                              |Estimated Median Income|\n",
      "+--------+-------------------------------------------------------------------------------------------------------+-----------------------+\n",
      "|90001   |Los Angeles (South Los Angeles), Florence-Graham                                                       |$33,887                |\n",
      "|90002   |Los Angeles (Southeast Los Angeles, Watts)                                                             |$30,413                |\n",
      "|90003   |Los Angeles (South Los Angeles, Southeast Los Angeles)                                                 |$30,805                |\n",
      "|90004   |Los Angeles (Hancock Park, Rampart Village, Virgil Village, Wilshire Center, Windsor Square)           |$40,612                |\n",
      "|90005   |Los Angeles (Hancock Park, Koreatown, Wilshire Center, Wilshire Park, Windsor Square)                  |$31,142                |\n",
      "|90006   |Los Angeles (Byzantine-Latino Quarter, Harvard Heights, Koreatown, Pico Heights)                       |$31,521                |\n",
      "|90007   |Los Angeles (Southeast Los Angeles, Univerity Park)                                                    |$22,304                |\n",
      "|90008   |Los Angeles (Baldwin Hills, Crenshaw, Leimert Park)                                                    |$36,564                |\n",
      "|90010   |Los Angeles (Hancock Park, Wilshire Center, Windsor Square)                                            |$45,786                |\n",
      "|90011   |Los Angeles (Southeast Los Angeles)                                                                    |$30,251                |\n",
      "|90012   |Los Angeles (Downtown Civic Center, Chinatown, Arts District, Bunker Hill, Historic Core, Little Tokyo)|$31,576                |\n",
      "|90013   |Los Angeles (Downtown Central, Downtown Fashion District)                                              |$19,887                |\n",
      "|90014   |Los Angeles (Downtown Historic Core, Arts District)                                                    |$23,642                |\n",
      "|90015   |Los Angeles (Dowtown Fashion District, South Park-South)                                               |$29,684                |\n",
      "|90016   |Los Angeles (West Adams)                                                                               |$38,330                |\n",
      "|90017   |Los Angeles (Downtown Bunker Hill, City West, South Park-North)                                        |$22,754                |\n",
      "|90018   |Los Angeles (Jefferson Park, Leimert Park)                                                             |$33,864                |\n",
      "|90019   |Los Angeles (Arlington Heights, Country Club Park, Mid-City)                                           |$46,571                |\n",
      "|90020   |Los Angeles (Hancock Park, Western Wilton, Wilshire Center, Windsor Square)                            |$38,849                |\n",
      "|90021   |Los Angeles (Downtown Fashion District, Downtown Southeast)                                            |$12,813                |\n",
      "+--------+-------------------------------------------------------------------------------------------------------+-----------------------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "income_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, encoding=\"UTF-8\")\n",
    "print(\"Preview of the Income DataFrame:\")\n",
    "income_df.show(20, truncate=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "1670fa50-a4b6-429c-a777-70bafb703fbd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Income DataFrame:\n",
      "+-----+-------------------------------------------------------------------------------------------------------+-------+\n",
      "|ZIP  |Community                                                                                              |Income |\n",
      "+-----+-------------------------------------------------------------------------------------------------------+-------+\n",
      "|90001|Los Angeles (South Los Angeles), Florence-Graham                                                       |33887.0|\n",
      "|90002|Los Angeles (Southeast Los Angeles, Watts)                                                             |30413.0|\n",
      "|90003|Los Angeles (South Los Angeles, Southeast Los Angeles)                                                 |30805.0|\n",
      "|90004|Los Angeles (Hancock Park, Rampart Village, Virgil Village, Wilshire Center, Windsor Square)           |40612.0|\n",
      "|90005|Los Angeles (Hancock Park, Koreatown, Wilshire Center, Wilshire Park, Windsor Square)                  |31142.0|\n",
      "|90006|Los Angeles (Byzantine-Latino Quarter, Harvard Heights, Koreatown, Pico Heights)                       |31521.0|\n",
      "|90007|Los Angeles (Southeast Los Angeles, Univerity Park)                                                    |22304.0|\n",
      "|90008|Los Angeles (Baldwin Hills, Crenshaw, Leimert Park)                                                    |36564.0|\n",
      "|90010|Los Angeles (Hancock Park, Wilshire Center, Windsor Square)                                            |45786.0|\n",
      "|90011|Los Angeles (Southeast Los Angeles)                                                                    |30251.0|\n",
      "|90012|Los Angeles (Downtown Civic Center, Chinatown, Arts District, Bunker Hill, Historic Core, Little Tokyo)|31576.0|\n",
      "|90013|Los Angeles (Downtown Central, Downtown Fashion District)                                              |19887.0|\n",
      "|90014|Los Angeles (Downtown Historic Core, Arts District)                                                    |23642.0|\n",
      "|90015|Los Angeles (Dowtown Fashion District, South Park-South)                                               |29684.0|\n",
      "|90016|Los Angeles (West Adams)                                                                               |38330.0|\n",
      "|90017|Los Angeles (Downtown Bunker Hill, City West, South Park-North)                                        |22754.0|\n",
      "|90018|Los Angeles (Jefferson Park, Leimert Park)                                                             |33864.0|\n",
      "|90019|Los Angeles (Arlington Heights, Country Club Park, Mid-City)                                           |46571.0|\n",
      "|90020|Los Angeles (Hancock Park, Western Wilton, Wilshire Center, Windsor Square)                            |38849.0|\n",
      "|90021|Los Angeles (Downtown Fashion District, Downtown Southeast)                                            |12813.0|\n",
      "+-----+-------------------------------------------------------------------------------------------------------+-------+\n",
      "only showing top 20 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, trim, regexp_replace, lpad\n",
    "\n",
    "# Correctly load the CSV with header information\n",
    "income_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, encoding=\"UTF-8\")\n",
    "\n",
    "# Rename columns for consistency\n",
    "income_df = income_df.withColumnRenamed(\"Zip Code\", \"ZIP\").withColumnRenamed(\"Estimated Median Income\", \"Income\")\n",
    "\n",
    "# Clean ZIP and Income columns\n",
    "income_df = income_df.withColumn(\"ZIP\", lpad(trim(col(\"ZIP\")), 5, \"0\")) \\\n",
    "    .withColumn(\"Income\", regexp_replace(col(\"Income\"), \"[^0-9.]\", \"\").cast(\"double\"))\n",
    "\n",
    "# Preview the cleaned DataFrame\n",
    "print(\"Cleaned Income DataFrame:\")\n",
    "income_df.show(20, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "0bb6b719-381d-4db3-b8a7-f3e272c8cc9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matched ZIP Codes and Incomes:\n",
      "+-----+--------+---------------------+\n",
      "|ZIP  |Income  |Neighborhood         |\n",
      "+-----+--------+---------------------+\n",
      "|90732|84679.0 |San Pedro            |\n",
      "|91789|93301.0 |Diamond Bar          |\n",
      "|90275|118790.0|Rancho Palos Verdes  |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90732|84679.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90803|75197.0 |Long Beach           |\n",
      "|90275|118790.0|Rancho Palos Verdes  |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90803|75197.0 |Long Beach           |\n",
      "|90803|75197.0 |Long Beach           |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90803|75197.0 |Long Beach           |\n",
      "|90731|50879.0 |Harbor City          |\n",
      "|90274|157465.0|Rolling Hills Estates|\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90803|75197.0 |Long Beach           |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90744|41569.0 |Wilmington           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90744|41569.0 |Wilmington           |\n",
      "|93544|56205.0 |Llano                |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90744|41569.0 |Wilmington           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90744|41569.0 |Wilmington           |\n",
      "|90744|41569.0 |Wilmington           |\n",
      "|90016|38330.0 |Baldwin Hills        |\n",
      "|90275|118790.0|Rancho Palos Verdes  |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90732|84679.0 |San Pedro            |\n",
      "|90802|42829.0 |Long Beach           |\n",
      "|90731|50879.0 |San Pedro            |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90274|157465.0|Palos Verdes Estates |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90274|157465.0|Rolling Hills        |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90274|157465.0|Rolling Hills        |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90275|118790.0|Rancho Palos Verdes  |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90814|55116.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90744|41569.0 |Wilmington           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90275|118790.0|Rancho Palos Verdes  |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90274|157465.0|Palos Verdes Estates |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90744|41569.0 |Wilmington           |\n",
      "|90274|157465.0|Rolling Hills        |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90813|30936.0 |Long Beach           |\n",
      "|90804|44877.0 |Long Beach           |\n",
      "|90275|118790.0|Rancho Palos Verdes  |\n",
      "+-----+--------+---------------------+\n",
      "only showing top 100 rows\n",
      "\n",
      "Neighborhood Stats:\n",
      "+---------------------+-----------+---------------+------------------+\n",
      "|Neighborhood         |TotalIncome|TotalPopulation|IncomePerPerson   |\n",
      "+---------------------+-----------+---------------+------------------+\n",
      "|Culver City          |5.5268813E7|38883          |1421.4132911555178|\n",
      "|North Lancaster      |8949016.0  |1101           |8128.079927338783 |\n",
      "|Rosewood/East Gardena|1812404.0  |1164           |1557.0481099656358|\n",
      "|East Rancho Dominguez|8517880.0  |15135          |562.793524942187  |\n",
      "|Toluca Terrace       |387992.0   |1301           |298.2259800153728 |\n",
      "|Elysian Park         |3550350.0  |5267           |674.0744256692615 |\n",
      "|Longwood             |1073240.0  |4210           |254.92636579572448|\n",
      "|Pico Rivera          |4.5685306E7|62942          |725.8318134155254 |\n",
      "|Malibu               |3.905902E7 |12645          |3088.8904705417162|\n",
      "|Green Meadows        |5442076.0  |19821          |274.5611220422784 |\n",
      "|Hacienda Heights     |4.0791934E7|53594          |761.1287457551218 |\n",
      "|Cadillac-Corning     |1560630.0  |6665           |234.15303825956488|\n",
      "|West Puente Valley   |3870306.0  |9657           |400.7772600186393 |\n",
      "|Montebello           |3.5683535E7|62500          |570.93656         |\n",
      "|Mid-city             |6426798.0  |14339          |448.2040588604505 |\n",
      "|Lake Manor           |3272959.0  |1600           |2045.599375       |\n",
      "|Hawaiian Gardens     |4511963.0  |14254          |316.54012908657216|\n",
      "|Lincoln Heights      |1.1055134E7|31144          |354.9683406113537 |\n",
      "|Westlake Village     |1.9970593E7|8270           |2414.823821039903 |\n",
      "|Van Nuys             |1.6435468E7|86019          |191.06788035201527|\n",
      "+---------------------+-----------+---------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "3 Richest Neighborhoods:\n",
      "+----------------+-----------+---------------+------------------+\n",
      "|Neighborhood    |TotalIncome|TotalPopulation|IncomePerPerson   |\n",
      "+----------------+-----------+---------------+------------------+\n",
      "|Whittier Narrows|5310951.0  |13             |408534.6923076923 |\n",
      "|Industry        |3.4025919E7|219            |155369.49315068492|\n",
      "|Franklin Canyon |1307043.0  |11             |118822.09090909091|\n",
      "+----------------+-----------+---------------+------------------+\n",
      "\n",
      "3 Poorest Neighborhoods:\n",
      "+-----------------+-----------+---------------+-----------------+\n",
      "|Neighborhood     |TotalIncome|TotalPopulation|IncomePerPerson  |\n",
      "+-----------------+-----------+---------------+-----------------+\n",
      "|Bandini Islands  |258262.0   |0              |NULL             |\n",
      "|Westlake         |3743182.0  |54638          |68.50876679234233|\n",
      "|Little Bangladesh|2192505.0  |26415          |83.0022714366837 |\n",
      "+-----------------+-----------+---------------+-----------------+"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, lpad, trim, sum as _sum, when\n",
    "\n",
    "# Ensure ZIP codes in GeoJSON are formatted correctly\n",
    "geojson_flat_df = geojson_flat_df.withColumn(\"ZIP\", lpad(trim(col(\"ZIP\")), 5, \"0\"))\n",
    "\n",
    "# Perform the join on ZIP codes\n",
    "matched_zips_df = geojson_flat_df.join(income_df, on=\"ZIP\", how=\"inner\")\n",
    "\n",
    "# Check the matched ZIP codes and incomes\n",
    "print(\"Matched ZIP Codes and Incomes:\")\n",
    "matched_zips_df.select(\"ZIP\", \"Income\", \"Neighborhood\").show(100, truncate=False)\n",
    "\n",
    "# Aggregate income and population by neighborhood\n",
    "neighborhood_stats_df = matched_zips_df.groupBy(\"Neighborhood\").agg(\n",
    "    _sum(\"Income\").alias(\"TotalIncome\"),\n",
    "    _sum(\"Population\").alias(\"TotalPopulation\")\n",
    ")\n",
    "\n",
    "# Calculate income per person\n",
    "neighborhood_stats_df = neighborhood_stats_df.withColumn(\n",
    "    \"IncomePerPerson\",\n",
    "    when(col(\"TotalPopulation\") > 0, col(\"TotalIncome\") / col(\"TotalPopulation\")).otherwise(None)\n",
    ")\n",
    "\n",
    "# Show final neighborhood stats\n",
    "print(\"Neighborhood Stats:\")\n",
    "neighborhood_stats_df.show(truncate=False)\n",
    "\n",
    "# Find the 3 richest neighborhoods\n",
    "richest_neighborhoods_df = neighborhood_stats_df.orderBy(col(\"IncomePerPerson\").desc()).limit(3)\n",
    "print(\"3 Richest Neighborhoods:\")\n",
    "richest_neighborhoods_df.show(truncate=False)\n",
    "\n",
    "# Find the 3 poorest neighborhoods\n",
    "poorest_neighborhoods_df = neighborhood_stats_df.orderBy(col(\"IncomePerPerson\").asc()).limit(3)\n",
    "print(\"3 Poorest Neighborhoods:\")\n",
    "poorest_neighborhoods_df.show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "661a18b1-a20e-4f4a-b401-128597967594",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.appName(\"CrimeDataUnion\").getOrCreate()\n",
    "\n",
    "# File paths for the two datasets\n",
    "crime_data_2010_to_2019_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\"\n",
    "crime_data_2020_to_present_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "\n",
    "# Step 1: Load the two crime datasets\n",
    "crime_data_2010_to_2019_df = spark.read.csv(crime_data_2010_to_2019_path, header=True, inferSchema=True)\n",
    "crime_data_2020_to_present_df = spark.read.csv(crime_data_2020_to_present_path, header=True, inferSchema=True)\n",
    "\n",
    "# Step 2: Union the two datasets\n",
    "crimes_df = crime_data_2010_to_2019_df.union(crime_data_2020_to_present_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "efd5dd48-a80a-4fd0-9120-c450ffd05524",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "Session 2741 unexpectedly reached final status 'dead'. See logs:\n",
      "stdout: \n",
      "\n",
      "stderr: \n",
      "25/01/16 11:33:04 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "25/01/16 11:33:04 INFO DefaultNoHARMFailoverProxyProvider: Connecting to ResourceManager at ip-192-168-1-36.eu-central-1.compute.internal/192.168.1.36:8032\n",
      "25/01/16 11:33:05 INFO Configuration: resource-types.xml not found\n",
      "25/01/16 11:33:05 INFO ResourceUtils: Unable to find 'resource-types.xml'.\n",
      "25/01/16 11:33:05 INFO Client: Verifying our application has not requested more than the maximum memory capability of the cluster (12288 MB per container)\n",
      "25/01/16 11:33:05 INFO Client: Will allocate AM container, with 1384 MB memory including 384 MB overhead\n",
      "25/01/16 11:33:05 INFO Client: Setting up container launch context for our AM\n",
      "25/01/16 11:33:05 INFO Client: Setting up the launch environment for our AM container\n",
      "25/01/16 11:33:05 INFO Client: Preparing resources for our AM container\n",
      "25/01/16 11:33:05 WARN Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\n",
      "25/01/16 11:33:06 INFO Client: Uploading resource file:/mnt/tmp/spark-c941fed4-b19d-41d4-901a-3762f829d228/__spark_libs__11079099232206433566.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/__spark_libs__11079099232206433566.zip\n",
      "25/01/16 11:33:07 INFO Client: Uploading resource file:/jars/sedona-spark-shaded-3.5_2.12-1.6.1.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/sedona-spark-shaded-3.5_2.12-1.6.1.jar\n",
      "25/01/16 11:33:07 INFO Client: Uploading resource file:/jars/geotools-wrapper-1.6.1-28.2.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/geotools-wrapper-1.6.1-28.2.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/kryo-shaded-4.0.2.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/kryo-shaded-4.0.2.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-api-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/livy-api-0.8.0-incubating.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-rsc-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/livy-rsc-0.8.0-incubating.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/livy-thriftserver-session-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/livy-thriftserver-session-0.8.0-incubating.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/minlog-1.3.0.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/minlog-1.3.0.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-all-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-all-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-buffer-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-buffer-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-dns-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-dns-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-haproxy-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-haproxy-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-http-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-http-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-http2-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-http2-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-memcache-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-memcache-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-mqtt-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-mqtt-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-redis-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-redis-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-smtp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-smtp-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-socks-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-socks-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-stomp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-stomp-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-codec-xml-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-codec-xml-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-common-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-common-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-handler-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-proxy-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-handler-proxy-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-handler-ssl-ocsp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-handler-ssl-ocsp-4.1.100.Final.jar\n",
      "25/01/16 11:33:08 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-resolver-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-resolver-dns-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-classes-macos-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-resolver-dns-classes-macos-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-resolver-dns-native-macos-4.1.100.Final-osx-aarch_64.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-resolver-dns-native-macos-4.1.100.Final-osx-x86_64.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-classes-epoll-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-classes-epoll-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-classes-kqueue-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-classes-kqueue-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-native-epoll-4.1.100.Final-linux-aarch_64.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-native-epoll-4.1.100.Final-linux-x86_64.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-native-kqueue-4.1.100.Final-osx-aarch_64.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-native-kqueue-4.1.100.Final-osx-x86_64.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-native-unix-common-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-native-unix-common-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-rxtx-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-rxtx-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-sctp-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-sctp-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/netty-transport-udt-4.1.100.Final.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/netty-transport-udt-4.1.100.Final.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/rsc-jars/objenesis-2.5.1.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/objenesis-2.5.1.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/commons-codec-1.9.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/commons-codec-1.9.jar\n",
      "25/01/16 11:33:09 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/kryo-shaded-4.0.2.jar added multiple times to distributed cache\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-core_2.12-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/livy-core_2.12-0.8.0-incubating.jar\n",
      "25/01/16 11:33:09 INFO Client: Uploading resource file:/usr/lib/livy/repl_2.12-jars/livy-repl_2.12-0.8.0-incubating.jar -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/livy-repl_2.12-0.8.0-incubating.jar\n",
      "25/01/16 11:33:10 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/minlog-1.3.0.jar added multiple times to distributed cache\n",
      "25/01/16 11:33:10 WARN Client: Same name resource file:///usr/lib/livy/repl_2.12-jars/objenesis-2.5.1.jar added multiple times to distributed cache\n",
      "25/01/16 11:33:10 INFO Client: Uploading resource file:/etc/spark/conf.dist/hive-site.xml -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/hive-site.xml\n",
      "25/01/16 11:33:10 INFO Client: Uploading resource file:/etc/hudi/conf.dist/hudi-defaults.conf -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/hudi-defaults.conf\n",
      "25/01/16 11:33:10 INFO Client: Uploading resource file:/usr/lib/spark/R/lib/sparkr.zip#sparkr -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/sparkr.zip\n",
      "25/01/16 11:33:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/pyspark.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/pyspark.zip\n",
      "25/01/16 11:33:10 INFO Client: Uploading resource file:/usr/lib/spark/python/lib/py4j-0.10.9.7-src.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/py4j-0.10.9.7-src.zip\n",
      "25/01/16 11:33:10 INFO Client: Uploading resource file:/mnt/tmp/spark-c941fed4-b19d-41d4-901a-3762f829d228/__spark_conf__1295081478675690513.zip -> hdfs://ip-192-168-1-36.eu-central-1.compute.internal:8020/user/livy/.sparkStaging/application_1732639283265_2700/__spark_conf__.zip\n",
      "25/01/16 11:33:10 INFO SecurityManager: Changing view acls to: livy\n",
      "25/01/16 11:33:10 INFO SecurityManager: Changing modify acls to: livy\n",
      "25/01/16 11:33:10 INFO SecurityManager: Changing view acls groups to: \n",
      "25/01/16 11:33:10 INFO SecurityManager: Changing modify acls groups to: \n",
      "25/01/16 11:33:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: livy; groups with view permissions: EMPTY; users with modify permissions: livy; groups with modify permissions: EMPTY\n",
      "25/01/16 11:33:10 INFO Client: Submitting application application_1732639283265_2700 to ResourceManager\n",
      "25/01/16 11:33:10 INFO YarnClientImpl: Submitted application application_1732639283265_2700\n",
      "25/01/16 11:33:10 INFO Client: Application report for application_1732639283265_2700 (state: ACCEPTED)\n",
      "25/01/16 11:33:10 INFO Client: \n",
      "\t client token: N/A\n",
      "\t diagnostics: [Thu Jan 16 11:33:10 +0000 2025] Application is Activated, waiting for resources to be assigned for AM.  Details : AM Partition = <DEFAULT_PARTITION> ; Partition Resource = <memory:196608, vCores:64> ; Queue's Absolute capacity = 100.0 % ; Queue's Absolute used capacity = 96.875 % ; Queue's Absolute max capacity = 100.0 % ; Queue's capacity (absolute resource) = <memory:196608, vCores:64> ; Queue's used capacity (absolute resource) = <memory:190464, vCores:80> ; Queue's max capacity (absolute resource) = <memory:196608, vCores:64> ; \n",
      "\t ApplicationMaster host: N/A\n",
      "\t ApplicationMaster RPC port: -1\n",
      "\t queue: root.default\n",
      "\t start time: 1737027190904\n",
      "\t final status: UNDEFINED\n",
      "\t tracking URL: http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_2700/\n",
      "\t user: livy\n",
      "25/01/16 11:33:10 INFO ShutdownHookManager: Shutdown hook called\n",
      "25/01/16 11:33:10 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-c941fed4-b19d-41d4-901a-3762f829d228\n",
      "25/01/16 11:33:10 INFO ShutdownHookManager: Deleting directory /mnt/tmp/spark-c05cd3c2-7617-462a-a142-522473846d72\n",
      "\n",
      "YARN Diagnostics: \n",
      "Application application_1732639283265_2700 failed 1 times (global limit =2; local limit is =1) due to AM Container for appattempt_1732639283265_2700_000001 exited with  exitCode: 137\n",
      "Failing this attempt.Diagnostics: [2025-01-16 19:17:46.661]Container killed on request. Exit code is 137\n",
      "[2025-01-16 19:17:46.661]Container exited with a non-zero exit code 137. \n",
      "[2025-01-16 19:17:46.661]Killed by external signal\n",
      "For more detailed output, check the application tracking page: http://ip-192-168-1-36.eu-central-1.compute.internal:8088/cluster/app/application_1732639283265_2700 Then click on links to logs of each attempt.\n",
      ". Failing the application.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, broadcast\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Step 1: Load Census GeoJSON as a GeoPandas DataFrame\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "geojson_gdf = gpd.read_file(geojson_path)\n",
    "\n",
    "# Step 2: Create Point Geometry for Crimes Data\n",
    "crimes_pdf = crimes_df.select(\"LAT\", \"LON\", \"DR_NO\", \"Crm Cd Desc\", \"Date Rptd\").toPandas()\n",
    "crimes_pdf[\"geometry\"] = crimes_pdf.apply(lambda row: Point(float(row[\"LON\"]), float(row[\"LAT\"])), axis=1)\n",
    "crimes_gdf = gpd.GeoDataFrame(crimes_pdf, geometry=\"geometry\")\n",
    "\n",
    "# Step 3: Perform Spatial Join\n",
    "crimes_with_neighborhoods = gpd.sjoin(crimes_gdf, geojson_gdf, how=\"left\", predicate=\"within\")\n",
    "\n",
    "# Step 4: Filter for Richest and Poorest Neighborhoods\n",
    "richest_neighborhoods = [\"Whittier Narrows\", \"Industry\", \"Franklin Canyon\"]\n",
    "poorest_neighborhoods = [\"Bandini Islands\", \"Westlake\", \"Little Bangladesh\"]\n",
    "\n",
    "# Crimes in Richest Neighborhoods\n",
    "richest_crimes = crimes_with_neighborhoods[crimes_with_neighborhoods[\"Neighborhood\"].isin(richest_neighborhoods)]\n",
    "print(\"Crimes in the 3 Richest Neighborhoods:\")\n",
    "print(richest_crimes[[\"DR_NO\", \"Neighborhood\", \"Crm Cd Desc\", \"Date Rptd\"]])\n",
    "\n",
    "# Crimes in Poorest Neighborhoods\n",
    "poorest_crimes = crimes_with_neighborhoods[crimes_with_neighborhoods[\"Neighborhood\"].isin(poorest_neighborhoods)]\n",
    "print(\"Crimes in the 3 Poorest Neighborhoods:\")\n",
    "print(poorest_crimes[[\"DR_NO\", \"Neighborhood\", \"Crm Cd Desc\", \"Date Rptd\"]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62bf6a-f6fa-4bf8-875c-ffac054a79de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
