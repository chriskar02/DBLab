{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea1f308-88c2-4bc0-858c-059c0cc74a37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314dacf4-a42e-4ff1-a74c-72cccf8fd093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a228252-405c-45e3-9b8e-44e3182fec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both Dataframe and RDD implementations are to use 4 spark executors\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query1 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03e83166-6544-4c45-89df-2fbd6459f7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|    AgeGroup|Count|\n",
      "+------------+-----+\n",
      "|      Adults|72610|\n",
      "|Young Adults|23472|\n",
      "|    Children|10724|\n",
      "|     Seniors| 3099|\n",
      "+------------+-----+\n",
      "\n",
      "Execution Time (DataFrame API): 19.31 seconds"
     ]
    }
   ],
   "source": [
    "# Query1 Dataframe implementation\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load Crime Data\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter for \"AGGRAVATED ASSAULT\"\n",
    "assault_data = crime_data.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# Age Groups\n",
    "categorized = assault_data.withColumn(\n",
    "    \"AgeGroup\",\n",
    "    when(col(\"Vict Age\") < 18, \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Seniors\")\n",
    ")\n",
    "\n",
    "# Group and count\n",
    "result_df = categorized.groupBy(\"AgeGroup\").agg(count(\"*\").alias(\"Count\")).orderBy(col(\"Count\").desc())\n",
    "\n",
    "#Show results\n",
    "result_df.show()\n",
    "\n",
    "# Stop timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution Time (DataFrame API): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5ccd071f-5afa-4402-8c4b-8241a9655034",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DR_NO: integer (nullable = true)\n",
      " |-- Date Rptd: string (nullable = true)\n",
      " |-- DATE OCC: string (nullable = true)\n",
      " |-- TIME OCC: integer (nullable = true)\n",
      " |-- AREA : integer (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Rpt Dist No: integer (nullable = true)\n",
      " |-- Part 1-2: integer (nullable = true)\n",
      " |-- Crm Cd: integer (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: integer (nullable = true)\n",
      " |-- Vict Sex: string (nullable = true)\n",
      " |-- Vict Descent: string (nullable = true)\n",
      " |-- Premis Cd: integer (nullable = true)\n",
      " |-- Premis Desc: string (nullable = true)\n",
      " |-- Weapon Used Cd: integer (nullable = true)\n",
      " |-- Weapon Desc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- Crm Cd 1: integer (nullable = true)\n",
      " |-- Crm Cd 2: integer (nullable = true)\n",
      " |-- Crm Cd 3: integer (nullable = true)\n",
      " |-- Crm Cd 4: integer (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- Cross Street: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)\n",
      "\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "|    DR_NO|           Date Rptd|            DATE OCC|TIME OCC|AREA |AREA NAME|Rpt Dist No|Part 1-2|Crm Cd|         Crm Cd Desc|       Mocodes|Vict Age|Vict Sex|Vict Descent|Premis Cd|         Premis Desc|Weapon Used Cd|         Weapon Desc|Status| Status Desc|Crm Cd 1|Crm Cd 2|Crm Cd 3|Crm Cd 4|            LOCATION|        Cross Street|    LAT|      LON|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "|  1307355|02/20/2010 12:00:...|02/20/2010 12:00:...|    1350|   13|   Newton|       1385|       2|   900|VIOLATION OF COUR...|0913 1814 2000|      48|       M|           H|      501|SINGLE FAMILY DWE...|          NULL|                NULL|    AA|Adult Arrest|     900|    NULL|    NULL|    NULL|300 E  GAGE      ...|                NULL|33.9825|-118.2695|\n",
      "| 11401303|09/13/2010 12:00:...|09/12/2010 12:00:...|      45|   14|  Pacific|       1485|       2|   740|VANDALISM - FELON...|          0329|       0|       M|           W|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     740|    NULL|    NULL|    NULL|SEPULVEDA        ...|MANCHESTER       ...|33.9599|-118.3962|\n",
      "| 70309629|08/09/2010 12:00:...|08/09/2010 12:00:...|    1515|   13|   Newton|       1324|       2|   946|OTHER MISCELLANEO...|          0344|       0|       M|           H|      103|               ALLEY|          NULL|                NULL|    IC| Invest Cont|     946|    NULL|    NULL|    NULL|1300 E  21ST     ...|                NULL|34.0224|-118.2524|\n",
      "| 90631215|01/05/2010 12:00:...|01/05/2010 12:00:...|     150|    6|Hollywood|        646|       2|   900|VIOLATION OF COUR...|1100 0400 1402|      47|       F|           W|      101|              STREET|           102|            HAND GUN|    IC| Invest Cont|     900|     998|    NULL|    NULL|CAHUENGA         ...|HOLLYWOOD        ...|34.1016|-118.3295|\n",
      "|100100501|01/03/2010 12:00:...|01/02/2010 12:00:...|    2100|    1|  Central|        176|       1|   122|     RAPE, ATTEMPTED|          0400|      47|       F|           H|      103|               ALLEY|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     122|    NULL|    NULL|    NULL|8TH              ...|SAN PEDRO        ...|34.0387|-118.2488|\n",
      "|100100506|01/05/2010 12:00:...|01/04/2010 12:00:...|    1650|    1|  Central|        162|       1|   442|SHOPLIFTING - PET...|     0344 1402|      23|       M|           B|      404|    DEPARTMENT STORE|          NULL|                NULL|    AA|Adult Arrest|     442|    NULL|    NULL|    NULL|700 W  7TH       ...|                NULL| 34.048|-118.2577|\n",
      "|100100508|01/08/2010 12:00:...|01/07/2010 12:00:...|    2005|    1|  Central|        182|       1|   330|BURGLARY FROM VEH...|          0344|      46|       M|           H|      101|              STREET|          NULL|                NULL|    IC| Invest Cont|     330|    NULL|    NULL|    NULL|PICO             ...|GRAND            ...|34.0389|-118.2643|\n",
      "|100100509|01/09/2010 12:00:...|01/08/2010 12:00:...|    2100|    1|  Central|        157|       1|   230|ASSAULT WITH DEAD...|          0416|      51|       M|           B|      710|       OTHER PREMISE|           500|UNKNOWN WEAPON/OT...|    AA|Adult Arrest|     230|    NULL|    NULL|    NULL|500    CROCKER   ...|                NULL|34.0435|-118.2427|\n",
      "|100100510|01/09/2010 12:00:...|01/09/2010 12:00:...|     230|    1|  Central|        171|       1|   230|ASSAULT WITH DEAD...|     0400 0416|      30|       M|           H|      108|         PARKING LOT|           400|STRONG-ARM (HANDS...|    IC| Invest Cont|     230|    NULL|    NULL|    NULL|800 W  OLYMPIC   ...|                NULL| 34.045| -118.264|\n",
      "|100100511|01/09/2010 12:00:...|01/06/2010 12:00:...|    2100|    1|  Central|        132|       1|   341|THEFT-GRAND ($950...|     0344 1402|      55|       M|           W|      710|       OTHER PREMISE|          NULL|                NULL|    IC| Invest Cont|     341|     998|    NULL|    NULL|200 S  OLIVE     ...|                NULL|34.0538|-118.2488|\n",
      "+---------+--------------------+--------------------+--------+-----+---------+-----------+--------+------+--------------------+--------------+--------+--------+------------+---------+--------------------+--------------+--------------------+------+------------+--------+--------+--------+--------+--------------------+--------------------+-------+---------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Show dataset schema to find indexes for RDD\n",
    "crime_data.printSchema()\n",
    "\n",
    "# Display sample rows\n",
    "crime_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0816e675-e0fc-416a-9d2a-48c3655e522f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adults: 72610\n",
      "Young Adults: 23472\n",
      "Children: 10724\n",
      "Seniors: 3099\n",
      "Execution Time (RDD API): 21.01 seconds"
     ]
    }
   ],
   "source": [
    "#Query 1 RDD implementation\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Load dataset as RDD\n",
    "crime_rdd = spark.sparkContext.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "# Extract header and filter it out\n",
    "header = crime_rdd.first()\n",
    "crime_rdd = crime_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Parse CSV rows\n",
    "def parse_csv(line):\n",
    "    return list(csv.reader([line]))[0]\n",
    "\n",
    "parsed_rdd = crime_rdd.map(parse_csv)\n",
    "\n",
    "# Filter for \"AGGRAVATED ASSAULT\"\n",
    "assault_rdd = parsed_rdd.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[9])\n",
    "\n",
    "# Age groups\n",
    "age_group_rdd = assault_rdd.map(lambda row: (\n",
    "    \"Children\" if int(row[11]) < 18 else\n",
    "    \"Young Adults\" if 18 <= int(row[11]) <= 24 else\n",
    "    \"Adults\" if 25 <= int(row[11]) <= 64 else\n",
    "    \"Seniors\"\n",
    "))\n",
    "\n",
    "# Group and count\n",
    "result_rdd = age_group_rdd.map(lambda group: (group, 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Show results\n",
    "for group, count in result_rdd.collect():\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# Stop timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution Time (RDD API): {elapsed_time:.2f} seconds\")\n",
    "\n",
    "#Stop spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b78c38f2-a79d-45f1-9d15-a5ac84131085",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DR_NO', 'Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA ', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'Crm Cd', 'Crm Cd Desc', 'Mocodes', 'Vict Age', 'Vict Sex', 'Vict Descent', 'Premis Cd', 'Premis Desc', 'Weapon Used Cd', 'Weapon Desc', 'Status', 'Status Desc', 'Crm Cd 1', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'LOCATION', 'Cross Street', 'LAT', 'LON']"
     ]
    }
   ],
   "source": [
    "print(crime_data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8cc55223-3f13-4c7e-b9d2-563cb0d94b48",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType([StructField('DR_NO', IntegerType(), True), StructField('Date Rptd', StringType(), True), StructField('DATE OCC', StringType(), True), StructField('TIME OCC', IntegerType(), True), StructField('AREA ', IntegerType(), True), StructField('AREA NAME', StringType(), True), StructField('Rpt Dist No', IntegerType(), True), StructField('Part 1-2', IntegerType(), True), StructField('Crm Cd', IntegerType(), True), StructField('Crm Cd Desc', StringType(), True), StructField('Mocodes', StringType(), True), StructField('Vict Age', IntegerType(), True), StructField('Vict Sex', StringType(), True), StructField('Vict Descent', StringType(), True), StructField('Premis Cd', IntegerType(), True), StructField('Premis Desc', StringType(), True), StructField('Weapon Used Cd', IntegerType(), True), StructField('Weapon Desc', StringType(), True), StructField('Status', StringType(), True), StructField('Status Desc', StringType(), True), StructField('Crm Cd 1', IntegerType(), True), StructField('Crm Cd 2', IntegerType(), True), StructField('Crm Cd 3', IntegerType(), True), StructField('Crm Cd 4', IntegerType(), True), StructField('LOCATION', StringType(), True), StructField('Cross Street', StringType(), True), StructField('LAT', DoubleType(), True), StructField('LON', DoubleType(), True)])"
     ]
    }
   ],
   "source": [
    "print(crime_data.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8d73b-0ffb-4457-8dc1-7feb85eeb62e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d891e09-2dcb-4d20-a00a-073e76044437",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### i) Data_Frame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af06644-66b7-49ff-bf89-822675ccdc17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, when, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b18b9f5-8a58-4295-bea9-d9cb26a0934d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|AREA NAME  |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "DataFrame API Execution Time: 65.09 seconds"
     ]
    }
   ],
   "source": [
    "# Aggregation logic\n",
    "aggregated = crime_data.groupBy(\n",
    "    expr(\"substring(`DATE OCC`, 7, 4)\").alias(\"YEAR\"),  # Extract year from DATE OCC\n",
    "    col(\"AREA NAME\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    sum(when(~col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), 1).otherwise(0)).alias(\"closed_cases\")  # Non-\"UNK\"/\"Invest Cont\" are closed\n",
    ").withColumn(\"closed_case_rate\", col(\"closed_cases\") / col(\"total_cases\"))\n",
    "\n",
    "# Define window specification for ranking within each year\n",
    "window_spec = Window.partitionBy(\"YEAR\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "# Assign rank and filter top 3 precincts per year\n",
    "ranked = aggregated.withColumn(\"ranking\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"ranking\") <= 3) \\\n",
    "    .orderBy(\"YEAR\", \"ranking\")\n",
    "\n",
    "# Measure the end time for the DataFrame API operations\n",
    "dataframe_api_end_time = time.time()\n",
    "\n",
    "# Count rows for showing all results\n",
    "row_count = ranked.count()\n",
    "\n",
    "# Show all rows in the output\n",
    "ranked.show(truncate=False, n=row_count)\n",
    "\n",
    "# Print DataFrame API execution time\n",
    "print(f\"DataFrame API Execution Time: {dataframe_api_end_time - dataframe_api_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdfdbf-f665-4f30-8b46-ba31433e42cb",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### ii) SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6123857-69d6-4398-81c8-834d1a443ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|Precinct   |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "SQL API Execution Time: 34.04 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "sql_api_start_time = time.time()\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query\n",
    "sql_query = \"\"\"\n",
    "    WITH Aggregated AS (\n",
    "        SELECT\n",
    "            SUBSTRING(`DATE OCC`, 7, 4) AS YEAR,\n",
    "            `AREA NAME` AS Precinct,\n",
    "            COUNT(*) AS total_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) / COUNT(*) AS closed_case_rate\n",
    "        FROM crime_data\n",
    "        GROUP BY SUBSTRING(`DATE OCC`, 7, 4), `AREA NAME`\n",
    "    ),\n",
    "    Ranked AS (\n",
    "        SELECT\n",
    "            YEAR,\n",
    "            Precinct,\n",
    "            total_cases,\n",
    "            closed_cases,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM Aggregated\n",
    "    )\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        Precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM Ranked\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY YEAR, ranking\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(sql_query)\n",
    "\n",
    "# Count rows to ensure all results are shown\n",
    "row_count = result.count()\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False, n=row_count)\n",
    "\n",
    "# End timer\n",
    "sql_api_end_time = time.time()\n",
    "\n",
    "# Print SQL API execution time\n",
    "print(f\"SQL API Execution Time: {sql_api_end_time - sql_api_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec51df-f14f-4d29-8f04-ebd1f9d6c5f6",
   "metadata": {},
   "source": [
    "csv to parquet transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcc1766b-8647-4cc2-8c0a-da1763410475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save Crime Data as Parquet\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2803ec8-24d3-47ff-b332-6e986dbcdf4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to s3://groups-bucket-dblab-905418150721/group28/query2/ in Parquet format."
     ]
    }
   ],
   "source": [
    "# Save as a single Parquet file to the specified S3 bucket\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group28/query2/\"\n",
    "crime_data.repartition(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path} in Parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ea579d8-50e2-40cf-8534-62e1ebe6d07d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|Precinct   |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "SQL API Execution Time: 15.60 seconds"
     ]
    }
   ],
   "source": [
    "#parquet solution\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "sql_api_start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crime Dataset in Parquet format\n",
    "crime_data = spark.read.parquet(\n",
    "    \"s3://groups-bucket-dblab-905418150721/group28/query2/part-00000-fea3c04b-7961-41ea-8e05-d62534cf766e-c000.snappy.parquet\"\n",
    ")\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query\n",
    "sql_query = \"\"\"\n",
    "    WITH Aggregated AS (\n",
    "        SELECT\n",
    "            SUBSTRING(`DATE OCC`, 7, 4) AS YEAR,\n",
    "            `AREA NAME` AS Precinct,\n",
    "            COUNT(*) AS total_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) / COUNT(*) AS closed_case_rate\n",
    "        FROM crime_data\n",
    "        GROUP BY SUBSTRING(`DATE OCC`, 7, 4), `AREA NAME`\n",
    "    ),\n",
    "    Ranked AS (\n",
    "        SELECT\n",
    "            YEAR,\n",
    "            Precinct,\n",
    "            total_cases,\n",
    "            closed_cases,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM Aggregated\n",
    "    )\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        Precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM Ranked\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY YEAR, ranking\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(sql_query)\n",
    "\n",
    "# Count rows to ensure all results are shown\n",
    "row_count = result.count()\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False, n=row_count)\n",
    "\n",
    "# End timer\n",
    "sql_api_end_time = time.time()\n",
    "\n",
    "# Print SQL API execution time\n",
    "print(f\"SQL API Execution Time: {sql_api_end_time - sql_api_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ab0dd3-90cd-4283-8b9a-3198653d8b37",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0ac7cf51-32d3-4c58-b822-a89c41d8bd71",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema of the GeoJSON file:\n",
      "root\n",
      " |-- _corrupt_record: string (nullable = true)\n",
      " |-- geometry: struct (nullable = true)\n",
      " |    |-- coordinates: array (nullable = true)\n",
      " |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |-- element: array (containsNull = true)\n",
      " |    |    |    |    |-- element: string (containsNull = true)\n",
      " |    |-- type: string (nullable = true)\n",
      " |-- properties: struct (nullable = true)\n",
      " |    |-- BG10: string (nullable = true)\n",
      " |    |-- BG10FIP10: string (nullable = true)\n",
      " |    |-- BG12: string (nullable = true)\n",
      " |    |-- CB10: string (nullable = true)\n",
      " |    |-- CEN_FIP13: string (nullable = true)\n",
      " |    |-- CITY: string (nullable = true)\n",
      " |    |-- CITYCOM: string (nullable = true)\n",
      " |    |-- COMM: string (nullable = true)\n",
      " |    |-- CT10: string (nullable = true)\n",
      " |    |-- CT12: string (nullable = true)\n",
      " |    |-- CTCB10: string (nullable = true)\n",
      " |    |-- HD_2012: long (nullable = true)\n",
      " |    |-- HD_NAME: string (nullable = true)\n",
      " |    |-- HOUSING10: long (nullable = true)\n",
      " |    |-- LA_FIP10: string (nullable = true)\n",
      " |    |-- OBJECTID: long (nullable = true)\n",
      " |    |-- POP_2010: long (nullable = true)\n",
      " |    |-- PUMA10: string (nullable = true)\n",
      " |    |-- SPA_2012: long (nullable = true)\n",
      " |    |-- SPA_NAME: string (nullable = true)\n",
      " |    |-- SUP_DIST: string (nullable = true)\n",
      " |    |-- SUP_LABEL: string (nullable = true)\n",
      " |    |-- ShapeSTArea: double (nullable = true)\n",
      " |    |-- ShapeSTLength: double (nullable = true)\n",
      " |    |-- ZCTA10: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      "\n",
      "Columns in the GeoJSON file:\n",
      "['_corrupt_record', 'geometry', 'properties', 'type']\n",
      "+----------------------------+--------+----------+----+\n",
      "|_corrupt_record             |geometry|properties|type|\n",
      "+----------------------------+--------+----------+----+\n",
      "|{                           |NULL    |NULL      |NULL|\n",
      "|\"type\": \"FeatureCollection\",|NULL    |NULL      |NULL|\n",
      "+----------------------------+--------+----------+----+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeoJSON Inspection with Sedona\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.6.1,org.apache.sedona:sedona-viz-3.0_2.12:1.6.1\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register Sedona functions\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Load GeoJSON file\n",
    "geojson_file_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "geojson_df = spark.read.format(\"json\").load(geojson_file_path)\n",
    "\n",
    "# Inspect the schema\n",
    "print(\"Schema of the GeoJSON file:\")\n",
    "geojson_df.printSchema()\n",
    "\n",
    "# Show the columns\n",
    "print(\"Columns in the GeoJSON file:\")\n",
    "print(geojson_df.columns)\n",
    "\n",
    "# Optionally, preview some data\n",
    "geojson_df.show(2, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8a0397b0-149c-4ad9-b6db-da469b59fb60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Load CSV and Inspect\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "57528fe8-ee79-4cb3-8cd3-d5cfe9e8afb7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the CSV file:\n",
      "['field', 'type', 'meaning']\n",
      "Schema of the CSV file:\n",
      "root\n",
      " |-- field: string (nullable = true)\n",
      " |-- type: string (nullable = true)\n",
      " |-- meaning: string (nullable = true)\n",
      "\n",
      "First two rows of the CSV file:\n",
      "+-------------+--------+-------------------------------------------------------------------------+\n",
      "|field        |type    |meaning                                                                  |\n",
      "+-------------+--------+-------------------------------------------------------------------------+\n",
      "|BG10         |string  |7-digit block group number (2010)                                        |\n",
      "|BG10FIP10    |string  |Combination of BG10 and LA_FIP10 (Los Angeles County FIP code)           |\n",
      "|BG12         |string  |7-digit block group number (2012)                                        |\n",
      "|CB10         |string  |4-digit census block number                                              |\n",
      "|CEN_FIP13    |string  |-                                                                        |\n",
      "|CITY         |string  |Incorporated city name                                                   |\n",
      "|CITYCOM      |string  |City/Community name label                                                |\n",
      "|COMM         |string  |Unincorporated area community name and LA City neighborhood              |\n",
      "|CT10         |string  |6-digit census tract number (2010)                                       |\n",
      "|CT12         |string  |6-digit census tract number (2012)                                       |\n",
      "|CTCB10       |string  |Combination of CT10 and CB10 (4-digit census block number)               |\n",
      "|HD_2012      |long    |Health District number                                                   |\n",
      "|HD_NAME      |string  |Health District name                                                     |\n",
      "|HOUSING10    |long    |2010 housing (PL 94-171 Redistricting Data Summary File - Total Housing) |\n",
      "|LA_FIP10     |string  |Los Angeles County FIP code                                              |\n",
      "|OBJECTID     |long    |-                                                                        |\n",
      "|POP_2010     |long    |Population (PL 94-171 Redistricting Data Summary File - Total Population)|\n",
      "|PUMA10       |string  |-                                                                        |\n",
      "|SPA_2012     |long    |Service Planning Area number (2012)                                      |\n",
      "|SPA_NAME     |string  |Service Planning Area name                                               |\n",
      "|SUP_DIST     |string  |Supervisorial District number                                            |\n",
      "|SUP_LABEL    |string  |Supervisorial District label                                             |\n",
      "|ShapeSTArea  |double  |-                                                                        |\n",
      "|ShapeSTLength|double  |-                                                                        |\n",
      "|ZCTA10       |string  |Zip Code Tabulation Area                                                 |\n",
      "|geometry     |geometry|Geometry of the block                                                    |\n",
      "+-------------+--------+-------------------------------------------------------------------------+"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks_fields.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect columns\n",
    "print(\"Columns in the CSV file:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Inspect schema\n",
    "print(\"Schema of the CSV file:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first two rows\n",
    "print(\"First two rows of the CSV file:\")\n",
    "df.show(30, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "86bfdff0-f56b-4b3a-bb98-5ae823a63f3e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the CSV file:\n",
      "['Zip Code', 'Community', 'Estimated Median Income']\n",
      "Schema of the CSV file:\n",
      "root\n",
      " |-- Zip Code: integer (nullable = true)\n",
      " |-- Community: string (nullable = true)\n",
      " |-- Estimated Median Income: string (nullable = true)\n",
      "\n",
      "First two rows of the CSV file:\n",
      "+--------+--------------------------------------------------------------------------------------------+-----------------------+\n",
      "|Zip Code|Community                                                                                   |Estimated Median Income|\n",
      "+--------+--------------------------------------------------------------------------------------------+-----------------------+\n",
      "|90001   |Los Angeles (South Los Angeles), Florence-Graham                                            |$33,887                |\n",
      "|90002   |Los Angeles (Southeast Los Angeles, Watts)                                                  |$30,413                |\n",
      "|90003   |Los Angeles (South Los Angeles, Southeast Los Angeles)                                      |$30,805                |\n",
      "|90004   |Los Angeles (Hancock Park, Rampart Village, Virgil Village, Wilshire Center, Windsor Square)|$40,612                |\n",
      "|90005   |Los Angeles (Hancock Park, Koreatown, Wilshire Center, Wilshire Park, Windsor Square)       |$31,142                |\n",
      "|90006   |Los Angeles (Byzantine-Latino Quarter, Harvard Heights, Koreatown, Pico Heights)            |$31,521                |\n",
      "|90007   |Los Angeles (Southeast Los Angeles, Univerity Park)                                         |$22,304                |\n",
      "|90008   |Los Angeles (Baldwin Hills, Crenshaw, Leimert Park)                                         |$36,564                |\n",
      "|90010   |Los Angeles (Hancock Park, Wilshire Center, Windsor Square)                                 |$45,786                |\n",
      "|90011   |Los Angeles (Southeast Los Angeles)                                                         |$30,251                |\n",
      "+--------+--------------------------------------------------------------------------------------------+-----------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect columns\n",
    "print(\"Columns in the CSV file:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Inspect schema\n",
    "print(\"Schema of the CSV file:\")\n",
    "df.printSchema()\n",
    "\n",
    "# Show the first two rows\n",
    "print(\"First two rows of the CSV file:\")\n",
    "df.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ca9dfb63-bd62-4a73-a0a3-1ae09bd07de3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, expr\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime and Income Analysis\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-python-adapter-3.0_2.12:1.6.1\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f73f9675-f36e-4a0a-ba66-24afb1baee05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "An error occurred while calling o1332.csv.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 47N9QW5R0CYY9KQK; S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==; Proxy: null), S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554)\n",
      "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1869)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.exists(EmrFileSystem.java:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$5(DataSource.scala:816)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:816)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:802)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 47N9QW5R0CYY9KQK; S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==; Proxy: null), S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420)\n",
      "\t... 24 more\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1732639283265_2560/container_1732639283265_2560_01_000002/pyspark.zip/pyspark/sql/readwriter.py\", line 740, in csv\n",
      "    return self._df(self._jreader.csv(self._spark._sc._jvm.PythonUtils.toSeq(path)))\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1732639283265_2560/container_1732639283265_2560_01_000002/py4j-0.10.9.7-src.zip/py4j/java_gateway.py\", line 1322, in __call__\n",
      "    return_value = get_return_value(\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1732639283265_2560/container_1732639283265_2560_01_000002/pyspark.zip/pyspark/errors/exceptions/captured.py\", line 179, in deco\n",
      "    return f(*a, **kw)\n",
      "  File \"/mnt/yarn/usercache/livy/appcache/application_1732639283265_2560/container_1732639283265_2560_01_000002/py4j-0.10.9.7-src.zip/py4j/protocol.py\", line 326, in get_return_value\n",
      "    raise Py4JJavaError(\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling o1332.csv.\n",
      ": java.io.IOException: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 47N9QW5R0CYY9KQK; S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==; Proxy: null), S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:429)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolderUsingFolderObject(Jets3tNativeFileSystemStore.java:255)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.isFolder(Jets3tNativeFileSystemStore.java:218)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.S3NativeFileSystem.getFileStatus(S3NativeFileSystem.java:554)\n",
      "\tat org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:1869)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.EmrFileSystem.exists(EmrFileSystem.java:408)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$5(DataSource.scala:816)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n",
      "\tat org.apache.spark.util.FileAccessContext$.withContext(FileAccessContext.scala:41)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4(DataSource.scala:816)\n",
      "\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$4$adapted(DataSource.scala:802)\n",
      "\tat org.apache.spark.util.ThreadUtils$.$anonfun$parmap$2(ThreadUtils.scala:384)\n",
      "\tat scala.concurrent.Future$.$anonfun$apply$1(Future.scala:659)\n",
      "\tat scala.util.Success.$anonfun$map$1(Try.scala:255)\n",
      "\tat scala.util.Success.map(Try.scala:213)\n",
      "\tat scala.concurrent.Future.$anonfun$map$1(Future.scala:292)\n",
      "\tat scala.concurrent.impl.Promise.liftedTree1$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.Promise.$anonfun$transform$1(Promise.scala:33)\n",
      "\tat scala.concurrent.impl.CallbackRunnable.run(Promise.scala:64)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask$RunnableExecuteAction.exec(ForkJoinTask.java:1395)\n",
      "\tat java.base/java.util.concurrent.ForkJoinTask.doExec(ForkJoinTask.java:373)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool$WorkQueue.topLevelExec(ForkJoinPool.java:1182)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.scan(ForkJoinPool.java:1655)\n",
      "\tat java.base/java.util.concurrent.ForkJoinPool.runWorker(ForkJoinPool.java:1622)\n",
      "\tat java.base/java.util.concurrent.ForkJoinWorkerThread.run(ForkJoinWorkerThread.java:165)\n",
      "Caused by: com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.model.AmazonS3Exception: The specified bucket does not exist (Service: Amazon S3; Status Code: 404; Error Code: NoSuchBucket; Request ID: 47N9QW5R0CYY9KQK; S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==; Proxy: null), S3 Extended Request ID: 1LqgmHq+T9qKUTRnlidaMgW9/DpvDk7an0BTbMVy33F/GTckERXYEp0Yf7fMT71i3uweUyN0e2iaoUbfCHvcEA==\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleErrorResponse(AmazonHttpClient.java:1879)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.handleServiceErrorResponse(AmazonHttpClient.java:1418)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeOneRequest(AmazonHttpClient.java:1387)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeHelper(AmazonHttpClient.java:1157)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.doExecute(AmazonHttpClient.java:814)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.executeWithTimer(AmazonHttpClient.java:781)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.execute(AmazonHttpClient.java:755)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutor.access$500(AmazonHttpClient.java:715)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient$RequestExecutionBuilderImpl.execute(AmazonHttpClient.java:697)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:561)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.http.AmazonHttpClient.execute(AmazonHttpClient.java:541)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5558)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.invoke(AmazonS3Client.java:5505)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.shaded.com.amazonaws.services.s3.AmazonS3Client.listObjectsV2(AmazonS3Client.java:1002)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:26)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.call.ListObjectsV2Call.perform(ListObjectsV2Call.java:12)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor$CallPerformer.call(GlobalS3Executor.java:114)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.executor.GlobalS3Executor.execute(GlobalS3Executor.java:141)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:196)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.invoke(AmazonS3LiteClient.java:191)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3.lite.AmazonS3LiteClient.listObjectsV2(AmazonS3LiteClient.java:75)\n",
      "\tat com.amazon.ws.emr.hadoop.fs.s3n.Jets3tNativeFileSystemStore.list(Jets3tNativeFileSystemStore.java:420)\n",
      "\t... 24 more\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, expr\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Crime and Income Analysis\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "# Census data\n",
    "geojson_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\"\n",
    "census_df = spark.read.format(\"json\").load(geojson_path)\n",
    "\n",
    "# Income data\n",
    "income_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\"\n",
    "income_df = spark.read.csv(income_path, header=True, inferSchema=True)\n",
    "\n",
    "# Crimes data\n",
    "crimes_path = \"s3://path-to/crimes.csv\"\n",
    "crimes_df = spark.read.csv(crimes_path, header=True, inferSchema=True)\n",
    "\n",
    "# --- STEP 1: Align column names ---\n",
    "# Census data: ZCTA10, Population\n",
    "census_selected = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\")\n",
    ")\n",
    "\n",
    "# Income data: Zip Code, Median Household Income\n",
    "income_selected = income_df.select(\n",
    "    col(\"Zip Code\"),\n",
    "    expr(\"cast(regexp_replace(`Estimated Median Income`, '\\\\$', '') as double)\").alias(\"Median Income\")\n",
    ")\n",
    "\n",
    "# Crimes data: Use AREA NAME for mapping\n",
    "# Assuming AREA NAME maps to ZIP codes (needs verification)\n",
    "crimes_aggregated = crimes_df.groupBy(\"AREA NAME\").agg(\n",
    "    count(\"*\").alias(\"Total Crimes\")\n",
    ").withColumnRenamed(\"AREA NAME\", \"Zip Code\")\n",
    "\n",
    "# --- STEP 2: Join datasets ---\n",
    "# Join Census with Income\n",
    "census_income_joined = census_selected.join(income_selected, on=\"Zip Code\", how=\"inner\")\n",
    "\n",
    "# Join the result with Crimes\n",
    "final_joined = census_income_joined.join(crimes_aggregated, on=\"Zip Code\", how=\"left\").fillna(0)\n",
    "\n",
    "# --- STEP 3: Perform calculations ---\n",
    "results = final_joined.withColumn(\n",
    "    \"Average Income Per Person\", col(\"Median Income\") / col(\"Population\")\n",
    ").withColumn(\n",
    "    \"Crime Ratio\", col(\"Total Crimes\") / col(\"Population\")\n",
    ").select(\n",
    "    col(\"Zip Code\"),\n",
    "    col(\"Average Income Per Person\"),\n",
    "    col(\"Crime Ratio\")\n",
    ")\n",
    "\n",
    "# --- STEP 4: Output the results ---\n",
    "results.show(truncate=False)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "4b494234-f443-41cb-bad9-90ba27aad7bb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the CSV file:\n",
      "['DR_NO', 'Date Rptd', 'DATE OCC', 'TIME OCC', 'AREA', 'AREA NAME', 'Rpt Dist No', 'Part 1-2', 'Crm Cd', 'Crm Cd Desc', 'Mocodes', 'Vict Age', 'Vict Sex', 'Vict Descent', 'Premis Cd', 'Premis Desc', 'Weapon Used Cd', 'Weapon Desc', 'Status', 'Status Desc', 'Crm Cd 1', 'Crm Cd 2', 'Crm Cd 3', 'Crm Cd 4', 'LOCATION', 'Cross Street', 'LAT', 'LON']\n",
      "Schema of the CSV file:\n",
      "root\n",
      " |-- DR_NO: integer (nullable = true)\n",
      " |-- Date Rptd: string (nullable = true)\n",
      " |-- DATE OCC: string (nullable = true)\n",
      " |-- TIME OCC: integer (nullable = true)\n",
      " |-- AREA: integer (nullable = true)\n",
      " |-- AREA NAME: string (nullable = true)\n",
      " |-- Rpt Dist No: integer (nullable = true)\n",
      " |-- Part 1-2: integer (nullable = true)\n",
      " |-- Crm Cd: integer (nullable = true)\n",
      " |-- Crm Cd Desc: string (nullable = true)\n",
      " |-- Mocodes: string (nullable = true)\n",
      " |-- Vict Age: integer (nullable = true)\n",
      " |-- Vict Sex: string (nullable = true)\n",
      " |-- Vict Descent: string (nullable = true)\n",
      " |-- Premis Cd: integer (nullable = true)\n",
      " |-- Premis Desc: string (nullable = true)\n",
      " |-- Weapon Used Cd: integer (nullable = true)\n",
      " |-- Weapon Desc: string (nullable = true)\n",
      " |-- Status: string (nullable = true)\n",
      " |-- Status Desc: string (nullable = true)\n",
      " |-- Crm Cd 1: integer (nullable = true)\n",
      " |-- Crm Cd 2: integer (nullable = true)\n",
      " |-- Crm Cd 3: integer (nullable = true)\n",
      " |-- Crm Cd 4: integer (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- Cross Street: string (nullable = true)\n",
      " |-- LAT: double (nullable = true)\n",
      " |-- LON: double (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect columns\n",
    "print(\"Columns in the CSV file:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Inspect schema\n",
    "print(\"Schema of the CSV file:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c664ef8-441a-4418-af83-1fedac12ecb1",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4b1db4-5815-4885-ae58-321c699dacb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, year\n",
    "import time\n",
    "\n",
    "def run_query4(spark, config_name):\n",
    "    # Start timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    # Load datasets\n",
    "    crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "    income_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "    descent_codes_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "    # Filter Crime Data for Year 2015\n",
    "    crime_2015_data = crime_data.filter(year(\"DATE OCC\") == 2015)\n",
    "\n",
    "    # Calculate top 3 and bottom 3 areas by income\n",
    "    top_areas = income_data.orderBy(col(\"Estimated Median Income\").desc()).limit(3)\n",
    "    bottom_areas = income_data.orderBy(col(\"Estimated Median Income\")).limit(3)\n",
    "\n",
    "    # Join descent codes with crime data\n",
    "    crime_with_descent_data = crime_2015_data.join(\n",
    "        descent_codes_data, \n",
    "        crime_2015_data[\"Vict Descent\"] == descent_codes_data[\"Vict Descent\"], \n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    # Join with income data\n",
    "    top_area_crime = crime_with_descent_data.join(\n",
    "        top_areas, \n",
    "        crime_with_descent_data[\"AREA NAME\"] == top_areas[\"Community\"], \n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    bottom_area_crime = crime_with_descent_data.join(\n",
    "        bottom_areas, \n",
    "        crime_with_descent_data[\"AREA NAME\"] == bottom_areas[\"Community\"], \n",
    "        \"inner\"\n",
    "    )\n",
    "\n",
    "    # Group by Victim Descent for Top 3 Areas\n",
    "    top_area_profile = top_area_crime.groupBy(\"Vict Descent Full\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "    # Group by Victim Descent for Bottom 3 Areas\n",
    "    bottom_area_profile = bottom_area_crime.groupBy(\"Vict Descent Full\").count().orderBy(col(\"count\").desc())\n",
    "\n",
    "    # Show Results\n",
    "    print(f\"Results for {config_name}\")\n",
    "    print(\"Top 3 Areas by Income - Victim Descent Profile\")\n",
    "    top_area_profile.show()\n",
    "\n",
    "    print(\"Bottom 3 Areas by Income - Victim Descent Profile\")\n",
    "    bottom_area_profile.show()\n",
    "    \n",
    "    # Stop timer and print elapsed time\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Execution Time for {config_name}: {elapsed_time:.2f} seconds\")\n",
    "\n",
    "\n",
    "# Main Script\n",
    "if __name__ == \"__main__\":\n",
    "    # Configurations\n",
    "    configurations = [\n",
    "        (\"1 Core, 2GB Memory\", 1, \"2g\"),\n",
    "        (\"2 Cores, 4GB Memory\", 2, \"4g\"),\n",
    "        (\"4 Cores, 8GB Memory\", 4, \"8g\")\n",
    "    ]\n",
    "\n",
    "    for config_name, cores, memory in configurations:\n",
    "        # Create SparkSession with specific configuration\n",
    "        spark = SparkSession.builder \\\n",
    "            .appName(f\"Query4 - {config_name}\") \\\n",
    "            .config(\"spark.executor.instances\", \"2\") \\\n",
    "            .config(\"spark.executor.cores\", cores) \\\n",
    "            .config(\"spark.executor.memory\", memory) \\\n",
    "            .getOrCreate()\n",
    "\n",
    "        # Run the query\n",
    "        run_query4(spark, config_name)\n",
    "\n",
    "        # Stop SparkSession\n",
    "        spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647757c-bcb4-43f3-8e25-d50d89bfe33c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### Query 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce7840-4027-44aa-90c4-1ae72eac9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, when, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11819c4e-18f4-4f90-9d30-2dab142d1e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the CSV file:\n",
      "['X', 'Y', 'FID', 'DIVISION', 'LOCATION', 'PREC']\n",
      "Schema of the CSV file:\n",
      "root\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      " |-- FID: integer (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- PREC: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect columns\n",
    "print(\"Columns in the CSV file:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Inspect schema\n",
    "print(\"Schema of the CSV file:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aea1b0-e9b7-48c0-8d73-54240f97213c",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 2 executors × 4 cores/8GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff827f61-6fda-4e8e-8aa2-b134061a0acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.02043779072548565 |\n",
      "|VAN NUYS        |211457          |0.028653154590629136|\n",
      "|WILSHIRE        |198150          |0.026312166557481587|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.01729162112331338 |\n",
      "|NORTH HOLLYWOOD |171159          |0.026115214222567722|\n",
      "|77TH STREET     |167323          |0.016584871496068188|\n",
      "|PACIFIC         |157468          |0.037495777088312074|\n",
      "|CENTRAL         |154474          |0.009868086849235298|\n",
      "|SOUTHEAST       |151999          |0.024150127195506455|\n",
      "|RAMPART         |149675          |0.014730484635455721|\n",
      "|TOPANGA         |147167          |0.03243890335156792 |\n",
      "|WEST VALLEY     |130933          |0.02897360719640746 |\n",
      "|HARBOR          |130206          |3.2997622866934675  |\n",
      "|FOOTHILL        |122515          |0.04125740608010438 |\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859795|\n",
      "|HOLLENBECK      |119329          |0.026407445235885316|\n",
      "|NEWTON          |109078          |0.015890866822603905|\n",
      "|MISSION         |109009          |0.035032007153604966|\n",
      "|NORTHEAST       |105687          |0.03907902069344001 |\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 56.37 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-sql-3.0_2.12:1.6.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e597d39-633e-4996-ba23-69e99542d912",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 4 executors × 2 cores/4GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "901e1710-1a32-4790-bade-85a902d818e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.020437790725485655|\n",
      "|VAN NUYS        |211457          |0.02865315459062913 |\n",
      "|WILSHIRE        |198150          |0.026312166557481583|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.01729162112331337 |\n",
      "|NORTH HOLLYWOOD |171159          |0.026115214222567722|\n",
      "|77TH STREET     |167323          |0.016584871496068194|\n",
      "|PACIFIC         |157468          |0.03749577708831207 |\n",
      "|CENTRAL         |154474          |0.0098680868492353  |\n",
      "|SOUTHEAST       |151999          |0.024150127195506462|\n",
      "|RAMPART         |149675          |0.014730484635455718|\n",
      "|TOPANGA         |147167          |0.03243890335156792 |\n",
      "|WEST VALLEY     |130933          |0.028973607196407465|\n",
      "|HARBOR          |130206          |3.2997622866934675  |\n",
      "|FOOTHILL        |122515          |0.041257406080104385|\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859802|\n",
      "|HOLLENBECK      |119329          |0.02640744523588532 |\n",
      "|NEWTON          |109078          |0.0158908668226039  |\n",
      "|MISSION         |109009          |0.03503200715360496 |\n",
      "|NORTHEAST       |105687          |0.039079020693440006|\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 46.34 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with specified resources\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9eb7b-22fa-45fb-a9cf-c1f1aa0635b2",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "### 8 executors × 1 core/2 GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61735c16-b7d8-4e55-8d3c-7422fdc72acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.02043779072548566 |\n",
      "|VAN NUYS        |211457          |0.028653154590629126|\n",
      "|WILSHIRE        |198150          |0.026312166557481583|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.017291621123313373|\n",
      "|NORTH HOLLYWOOD |171159          |0.02611521422256772 |\n",
      "|77TH STREET     |167323          |0.016584871496068188|\n",
      "|PACIFIC         |157468          |0.037495777088312074|\n",
      "|CENTRAL         |154474          |0.009868086849235298|\n",
      "|SOUTHEAST       |151999          |0.02415012719550645 |\n",
      "|RAMPART         |149675          |0.014730484635455718|\n",
      "|TOPANGA         |147167          |0.03243890335156791 |\n",
      "|WEST VALLEY     |130933          |0.02897360719640747 |\n",
      "|HARBOR          |130206          |3.299762286693468   |\n",
      "|FOOTHILL        |122515          |0.041257406080104385|\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859795|\n",
      "|HOLLENBECK      |119329          |0.026407445235885323|\n",
      "|NEWTON          |109078          |0.0158908668226039  |\n",
      "|MISSION         |109009          |0.03503200715360496 |\n",
      "|NORTHEAST       |105687          |0.039079020693440006|\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 16.25 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with specified resources\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dcd19b4-1c96-4ddf-ae01-da7aa3eb6018",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
