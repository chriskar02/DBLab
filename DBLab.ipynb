{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ea1f308-88c2-4bc0-858c-059c0cc74a37",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "314dacf4-a42e-4ff1-a74c-72cccf8fd093",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, when, count\n",
    "import time\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a228252-405c-45e3-9b8e-44e3182fec3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Both Dataframe and RDD implementations are to use 4 spark executors\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query1 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201f2413-e2fc-4fd4-8cc5-4b1b8b5e1030",
   "metadata": {},
   "source": [
    "#### Dataframe Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "03e83166-6544-4c45-89df-2fbd6459f7aa",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----+\n",
      "|    AgeGroup|Count|\n",
      "+------------+-----+\n",
      "|      Adults|72610|\n",
      "|Young Adults|23472|\n",
      "|    Children|10724|\n",
      "|     Seniors| 3099|\n",
      "+------------+-----+\n",
      "\n",
      "Execution Time (DataFrame API): 19.31 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load Crime Data\n",
    "crime_data = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Filter for \"AGGRAVATED ASSAULT\"\n",
    "assault_data = crime_data.filter(col(\"Crm Cd Desc\").contains(\"AGGRAVATED ASSAULT\"))\n",
    "\n",
    "# Age Groups\n",
    "categorized = assault_data.withColumn(\n",
    "    \"AgeGroup\",\n",
    "    when(col(\"Vict Age\") < 18, \"Children\")\n",
    "    .when((col(\"Vict Age\") >= 18) & (col(\"Vict Age\") <= 24), \"Young Adults\")\n",
    "    .when((col(\"Vict Age\") >= 25) & (col(\"Vict Age\") <= 64), \"Adults\")\n",
    "    .when(col(\"Vict Age\") > 64, \"Seniors\")\n",
    ")\n",
    "\n",
    "# Group and count\n",
    "result_df = categorized.groupBy(\"AgeGroup\").agg(count(\"*\").alias(\"Count\")).orderBy(col(\"Count\").desc())\n",
    "\n",
    "# Show results\n",
    "result_df.show()\n",
    "\n",
    "# Stop timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution Time (DataFrame API): {elapsed_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3d3508f-d96e-4b65-a1da-03ef6b08332f",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### RDD Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0816e675-e0fc-416a-9d2a-48c3655e522f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adults: 72610\n",
      "Young Adults: 23472\n",
      "Children: 10724\n",
      "Seniors: 3099\n",
      "Execution Time (RDD API): 21.01 seconds"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "# Load dataset as RDD\n",
    "crime_rdd = spark.sparkContext.textFile(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\")\n",
    "\n",
    "# Extract header and filter it out\n",
    "header = crime_rdd.first()\n",
    "crime_rdd = crime_rdd.filter(lambda row: row != header)\n",
    "\n",
    "# Parse CSV rows\n",
    "\n",
    "\n",
    "def parse_csv(line):\n",
    "    return list(csv.reader([line]))[0]\n",
    "\n",
    "\n",
    "parsed_rdd = crime_rdd.map(parse_csv)\n",
    "\n",
    "# Filter for \"AGGRAVATED ASSAULT\"\n",
    "assault_rdd = parsed_rdd.filter(lambda row: \"AGGRAVATED ASSAULT\" in row[9])\n",
    "\n",
    "# Age groups\n",
    "age_group_rdd = assault_rdd.map(lambda row: (\n",
    "    \"Children\" if int(row[11]) < 18 else\n",
    "    \"Young Adults\" if 18 <= int(row[11]) <= 24 else\n",
    "    \"Adults\" if 25 <= int(row[11]) <= 64 else\n",
    "    \"Seniors\"\n",
    "))\n",
    "\n",
    "# Group and count\n",
    "result_rdd = age_group_rdd.map(lambda group: (group, 1)).reduceByKey(lambda a, b: a + b).sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# Show results\n",
    "for group, count in result_rdd.collect():\n",
    "    print(f\"{group}: {count}\")\n",
    "\n",
    "# Stop timer and print elapsed time\n",
    "elapsed_time = time.time() - start_time\n",
    "print(f\"Execution Time (RDD API): {elapsed_time:.2f} seconds\")\n",
    "\n",
    "# Stop spark session\n",
    "spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cf8d73b-0ffb-4457-8dc1-7feb85eeb62e",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d891e09-2dcb-4d20-a00a-073e76044437",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### i) Data_Frame API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4af06644-66b7-49ff-bf89-822675ccdc17",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, when, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2b18b9f5-8a58-4295-bea9-d9cb26a0934d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|AREA NAME  |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "DataFrame API Execution Time: 65.09 seconds"
     ]
    }
   ],
   "source": [
    "# Aggregation logic\n",
    "aggregated = crime_data.groupBy(\n",
    "    expr(\"substring(`DATE OCC`, 7, 4)\").alias(\"YEAR\"),  # Extract year from DATE OCC\n",
    "    col(\"AREA NAME\")\n",
    ").agg(\n",
    "    count(\"*\").alias(\"total_cases\"),\n",
    "    sum(when(~col(\"Status Desc\").isin(\"UNK\", \"Invest Cont\"), 1).otherwise(0)).alias(\"closed_cases\")  # Non-\"UNK\"/\"Invest Cont\" are closed\n",
    ").withColumn(\"closed_case_rate\", col(\"closed_cases\") / col(\"total_cases\"))\n",
    "\n",
    "# Define window specification for ranking within each year\n",
    "window_spec = Window.partitionBy(\"YEAR\").orderBy(col(\"closed_case_rate\").desc())\n",
    "\n",
    "# Assign rank and filter top 3 precincts per year\n",
    "ranked = aggregated.withColumn(\"ranking\", row_number().over(window_spec)) \\\n",
    "    .filter(col(\"ranking\") <= 3) \\\n",
    "    .orderBy(\"YEAR\", \"ranking\")\n",
    "\n",
    "# Measure the end time for the DataFrame API operations\n",
    "dataframe_api_end_time = time.time()\n",
    "\n",
    "# Count rows for showing all results\n",
    "row_count = ranked.count()\n",
    "\n",
    "# Show all rows in the output\n",
    "ranked.show(truncate=False, n=row_count)\n",
    "\n",
    "# Print DataFrame API execution time\n",
    "print(f\"DataFrame API Execution Time: {dataframe_api_end_time - dataframe_api_start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27fdfdbf-f665-4f30-8b46-ba31433e42cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### ii) SQL API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d6123857-69d6-4398-81c8-834d1a443ae7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|Precinct   |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "SQL API Execution Time: 34.04 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "sql_api_start_time = time.time()\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query\n",
    "sql_query = \"\"\"\n",
    "    WITH Aggregated AS (\n",
    "        SELECT\n",
    "            SUBSTRING(`DATE OCC`, 7, 4) AS YEAR,\n",
    "            `AREA NAME` AS Precinct,\n",
    "            COUNT(*) AS total_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) / COUNT(*) AS closed_case_rate\n",
    "        FROM crime_data\n",
    "        GROUP BY SUBSTRING(`DATE OCC`, 7, 4), `AREA NAME`\n",
    "    ),\n",
    "    Ranked AS (\n",
    "        SELECT\n",
    "            YEAR,\n",
    "            Precinct,\n",
    "            total_cases,\n",
    "            closed_cases,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM Aggregated\n",
    "    )\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        Precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM Ranked\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY YEAR, ranking\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(sql_query)\n",
    "\n",
    "# Count rows to ensure all results are shown\n",
    "row_count = result.count()\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False, n=row_count)\n",
    "\n",
    "# End timer\n",
    "sql_api_end_time = time.time()\n",
    "\n",
    "# Print SQL API execution time\n",
    "print(f\"SQL API Execution Time: {sql_api_end_time - sql_api_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec51df-f14f-4d29-8f04-ebd1f9d6c5f6",
   "metadata": {},
   "source": [
    "csv to parquet transition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "dcc1766b-8647-4cc2-8c0a-da1763410475",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Save Crime Data as Parquet\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "c2803ec8-24d3-47ff-b332-6e986dbcdf4c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully saved to s3://groups-bucket-dblab-905418150721/group28/query2/ in Parquet format."
     ]
    }
   ],
   "source": [
    "# Save as a single Parquet file to the specified S3 bucket\n",
    "output_path = \"s3://groups-bucket-dblab-905418150721/group28/query2/\"\n",
    "crime_data.repartition(1).write.mode(\"overwrite\").parquet(output_path)\n",
    "\n",
    "print(f\"Data successfully saved to {output_path} in Parquet format.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8ea579d8-50e2-40cf-8534-62e1ebe6d07d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|YEAR|Precinct   |total_cases|closed_cases|closed_case_rate   |ranking|\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "|2010|Rampart    |8707       |2860        |0.32847134489491214|1      |\n",
      "|2010|Olympic    |8764       |2762        |0.3151528982199909 |2      |\n",
      "|2010|Harbor     |9598       |2818        |0.2936028339237341 |3      |\n",
      "|2011|Olympic    |7988       |2799        |0.35040060090135206|1      |\n",
      "|2011|Rampart    |8444       |2744        |0.324964471814306  |2      |\n",
      "|2011|Harbor     |9841       |2806        |0.2851336246316431 |3      |\n",
      "|2012|Olympic    |8543       |2930        |0.3429708533302119 |1      |\n",
      "|2012|Rampart    |8626       |2800        |0.3246000463714352 |2      |\n",
      "|2012|Harbor     |9441       |2786        |0.29509585848956676|3      |\n",
      "|2013|Olympic    |8305       |2789        |0.3358217940999398 |1      |\n",
      "|2013|Rampart    |8148       |2616        |0.32106038291605304|2      |\n",
      "|2013|Harbor     |8431       |2506        |0.29723638951488557|3      |\n",
      "|2014|Van Nuys   |9478       |3035        |0.320215235281705  |1      |\n",
      "|2014|West Valley|7953       |2505        |0.3149754809505847 |2      |\n",
      "|2014|Mission    |9976       |3115        |0.31224939855653566|3      |\n",
      "|2015|Van Nuys   |10485      |3383        |0.32265140677157844|1      |\n",
      "|2015|Mission    |10652      |3245        |0.30463762673676303|2      |\n",
      "|2015|Foothill   |7762       |2356        |0.3035300180365885 |3      |\n",
      "|2016|Van Nuys   |10508      |3383        |0.32194518462124094|1      |\n",
      "|2016|West Valley|9697       |3045        |0.3140146437042384 |2      |\n",
      "|2016|Foothill   |7991       |2390        |0.29908647228131646|3      |\n",
      "|2017|Van Nuys   |10825      |3470        |0.320554272517321  |1      |\n",
      "|2017|Mission    |10887      |3381        |0.31055387158996967|2      |\n",
      "|2017|Foothill   |8218       |2504        |0.30469700657094184|3      |\n",
      "|2018|Foothill   |8122       |2496        |0.30731346958877126|1      |\n",
      "|2018|Mission    |10206      |3136        |0.30727023319615915|2      |\n",
      "|2018|Van Nuys   |10486      |3031        |0.2890520694259012 |3      |\n",
      "|2019|Mission    |9197       |2826        |0.30727411112319236|1      |\n",
      "|2019|West Valley|8728       |2669        |0.3057974335472044 |2      |\n",
      "|2019|N Hollywood|11143      |3258        |0.2923808669119627 |3      |\n",
      "|2020|West Valley|8092       |2490        |0.30771131982204647|1      |\n",
      "|2020|Mission    |8481       |2557        |0.3014974649215894 |2      |\n",
      "|2020|Harbor     |8874       |2635        |0.29693486590038315|3      |\n",
      "|2021|Mission    |8236       |2497        |0.30318115590092276|1      |\n",
      "|2021|West Valley|8543       |2475        |0.28971087440009363|2      |\n",
      "|2021|Foothill   |7048       |1973        |0.27993757094211125|3      |\n",
      "|2022|West Valley|10284      |2729        |0.26536367172306496|1      |\n",
      "|2022|Harbor     |9196       |2422        |0.263375380600261  |2      |\n",
      "|2022|Topanga    |9461       |2482        |0.26234013317831095|3      |\n",
      "|2023|Foothill   |7156       |1915        |0.2676076020122974 |1      |\n",
      "|2023|Topanga    |9639       |2558        |0.2653802261645399 |2      |\n",
      "|2023|Mission    |8978       |2304        |0.2566273112051682 |3      |\n",
      "|2024|N Hollywood|6526       |1279        |0.19598528961078762|1      |\n",
      "|2024|Foothill   |3582       |667         |0.18620882188721385|2      |\n",
      "|2024|77th Street|6198       |1090        |0.17586318167150694|3      |\n",
      "+----+-----------+-----------+------------+-------------------+-------+\n",
      "\n",
      "SQL API Execution Time: 15.60 seconds"
     ]
    }
   ],
   "source": [
    "#parquet solution\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "sql_api_start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 SQL API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Crime Dataset in Parquet format\n",
    "crime_data = spark.read.parquet(\n",
    "    \"s3://groups-bucket-dblab-905418150721/group28/query2/part-00000-fea3c04b-7961-41ea-8e05-d62534cf766e-c000.snappy.parquet\"\n",
    ")\n",
    "\n",
    "# Register the DataFrame as a temporary SQL table\n",
    "crime_data.createOrReplaceTempView(\"crime_data\")\n",
    "\n",
    "# SQL query\n",
    "sql_query = \"\"\"\n",
    "    WITH Aggregated AS (\n",
    "        SELECT\n",
    "            SUBSTRING(`DATE OCC`, 7, 4) AS YEAR,\n",
    "            `AREA NAME` AS Precinct,\n",
    "            COUNT(*) AS total_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) AS closed_cases,\n",
    "            SUM(CASE WHEN `Status Desc` NOT IN ('UNK', 'Invest Cont') THEN 1 ELSE 0 END) / COUNT(*) AS closed_case_rate\n",
    "        FROM crime_data\n",
    "        GROUP BY SUBSTRING(`DATE OCC`, 7, 4), `AREA NAME`\n",
    "    ),\n",
    "    Ranked AS (\n",
    "        SELECT\n",
    "            YEAR,\n",
    "            Precinct,\n",
    "            total_cases,\n",
    "            closed_cases,\n",
    "            closed_case_rate,\n",
    "            ROW_NUMBER() OVER (PARTITION BY YEAR ORDER BY closed_case_rate DESC) AS ranking\n",
    "        FROM Aggregated\n",
    "    )\n",
    "    SELECT\n",
    "        YEAR,\n",
    "        Precinct,\n",
    "        total_cases,\n",
    "        closed_cases,\n",
    "        closed_case_rate,\n",
    "        ranking\n",
    "    FROM Ranked\n",
    "    WHERE ranking <= 3\n",
    "    ORDER BY YEAR, ranking\n",
    "\"\"\"\n",
    "\n",
    "# Execute the SQL query\n",
    "result = spark.sql(sql_query)\n",
    "\n",
    "# Count rows to ensure all results are shown\n",
    "row_count = result.count()\n",
    "\n",
    "# Show the results\n",
    "result.show(truncate=False, n=row_count)\n",
    "\n",
    "# End timer\n",
    "sql_api_end_time = time.time()\n",
    "\n",
    "# Print SQL API execution time\n",
    "print(f\"SQL API Execution Time: {sql_api_end_time - sql_api_start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6016180-3e1f-4ecf-aeb6-1cfdfafa4433",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "061d9333-acd0-4e91-b16c-7df1f9b3415a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3840</td><td>application_1732639283265_3780</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3780/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3780_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Parsed Logical Plan ==\n",
      "'Sort ['Crime Per Person DESC NULLS LAST], true\n",
      "+- Project [COMM#72, Total Population#208L, Crime Count#217L, Crime Per Person#249, Median Income#197, Income per Person#256]\n",
      "   +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, Crime Per Person#249, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "      +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249]\n",
      "         +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197]\n",
      "            +- Join Inner, (COMM#72 = COMM#241)\n",
      "               :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "               :  +- Join LeftOuter, (COMM#72 = COMM#227)\n",
      "               :     :- Aggregate [COMM#72], [COMM#72, sum(Population#70L) AS Total Population#208L, sum(Housing#71L) AS Total Housing#210L]\n",
      "               :     :  +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :     :     +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, geojson_to_wkt(Coordinates#73)#85 AS wkt_geometry#86]\n",
      "               :     :        +- Project [properties#63.ZCTA10 AS Zip Code#69, properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72, geometry#62.coordinates AS Coordinates#73]\n",
      "               :     :           +- Relation [_corrupt_record#61,geometry#62,properties#63,type#64] json\n",
      "               :     +- Aggregate [COMM#227], [COMM#227, count(DR_NO#119) AS Crime Count#217L]\n",
      "               :        +- Project [COMM#227, DR_NO#119]\n",
      "               :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "               :              :- SubqueryAlias cr\n",
      "               :              :  +- SubqueryAlias crime\n",
      "               :              :     +- View (`crime`, [DR_NO#119,LAT#145,LON#146,geometry#179])\n",
      "               :              :        +- Project [DR_NO#119, LAT#145, LON#146,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "               :              :           +- Project [DR_NO#119, LAT#145, LON#146]\n",
      "               :              :              +- Filter (substring(DATE OCC#121, 7, 4) = 2010)\n",
      "               :              :                 +- Relation [DR_NO#119,Date Rptd#120,DATE OCC#121,TIME OCC#122,AREA #123,AREA NAME#124,Rpt Dist No#125,Part 1-2#126,Crm Cd#127,Crm Cd Desc#128,Mocodes#129,Vict Age#130,Vict Sex#131,Vict Descent#132,Premis Cd#133,Premis Desc#134,Weapon Used Cd#135,Weapon Desc#136,Status#137,Status Desc#138,Crm Cd 1#139,Crm Cd 2#140,Crm Cd 3#141,Crm Cd 4#142,... 4 more fields] csv\n",
      "               :              +- SubqueryAlias c\n",
      "               :                 +- SubqueryAlias census\n",
      "               :                    +- View (`census`, [Zip Code#224,Population#225L,Housing#226L,COMM#227,Coordinates#228,wkt_geometry#86,geometry#93])\n",
      "               :                       +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :                          +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, geojson_to_wkt(Coordinates#228)#85 AS wkt_geometry#86]\n",
      "               :                             +- Project [properties#222.ZCTA10 AS Zip Code#224, properties#222.POP_2010 AS Population#225L, properties#222.HOUSING10 AS Housing#226L, properties#222.COMM AS COMM#227, geometry#221.coordinates AS Coordinates#228]\n",
      "               :                                +- Relation [_corrupt_record#220,geometry#221,properties#222,type#223] json\n",
      "               +- Aggregate [COMM#241], [COMM#241, round(avg(Estimated Median Income#48), 2) AS Median Income#197]\n",
      "                  +- Project [Zip Code#42, Estimated Median Income#48, COMM#241]\n",
      "                     +- Join Inner, (Zip Code#42 = cast(Zip Code#238 as int))\n",
      "                        :- Project [Zip Code#42, Estimated Median Income#48]\n",
      "                        :  +- Project [Zip Code#42, Community#43, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "                        :     +- Relation [Zip Code#42,Community#43,Estimated Median Income#44] csv\n",
      "                        +- Project [Zip Code#238, COMM#241]\n",
      "                           +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "                              +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, geojson_to_wkt(Coordinates#242)#85 AS wkt_geometry#86]\n",
      "                                 +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.POP_2010 AS Population#239L, properties#236.HOUSING10 AS Housing#240L, properties#236.COMM AS COMM#241, geometry#235.coordinates AS Coordinates#242]\n",
      "                                    +- Relation [_corrupt_record#234,geometry#235,properties#236,type#237] json\n",
      "\n",
      "== Analyzed Logical Plan ==\n",
      "COMM: string, Total Population: bigint, Crime Count: bigint, Crime Per Person: double, Median Income: double, Income per Person: double\n",
      "Sort [Crime Per Person#249 DESC NULLS LAST], true\n",
      "+- Project [COMM#72, Total Population#208L, Crime Count#217L, Crime Per Person#249, Median Income#197, Income per Person#256]\n",
      "   +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, Crime Per Person#249, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "      +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249]\n",
      "         +- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L, Median Income#197]\n",
      "            +- Join Inner, (COMM#72 = COMM#241)\n",
      "               :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "               :  +- Join LeftOuter, (COMM#72 = COMM#227)\n",
      "               :     :- Aggregate [COMM#72], [COMM#72, sum(Population#70L) AS Total Population#208L, sum(Housing#71L) AS Total Housing#210L]\n",
      "               :     :  +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :     :     +- Project [Zip Code#69, Population#70L, Housing#71L, COMM#72, Coordinates#73, geojson_to_wkt(Coordinates#73)#85 AS wkt_geometry#86]\n",
      "               :     :        +- Project [properties#63.ZCTA10 AS Zip Code#69, properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72, geometry#62.coordinates AS Coordinates#73]\n",
      "               :     :           +- Relation [_corrupt_record#61,geometry#62,properties#63,type#64] json\n",
      "               :     +- Aggregate [COMM#227], [COMM#227, count(DR_NO#119) AS Crime Count#217L]\n",
      "               :        +- Project [COMM#227, DR_NO#119]\n",
      "               :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "               :              :- SubqueryAlias cr\n",
      "               :              :  +- SubqueryAlias crime\n",
      "               :              :     +- View (`crime`, [DR_NO#119,LAT#145,LON#146,geometry#179])\n",
      "               :              :        +- Project [DR_NO#119, LAT#145, LON#146,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "               :              :           +- Project [DR_NO#119, LAT#145, LON#146]\n",
      "               :              :              +- Filter (substring(DATE OCC#121, 7, 4) = 2010)\n",
      "               :              :                 +- Relation [DR_NO#119,Date Rptd#120,DATE OCC#121,TIME OCC#122,AREA #123,AREA NAME#124,Rpt Dist No#125,Part 1-2#126,Crm Cd#127,Crm Cd Desc#128,Mocodes#129,Vict Age#130,Vict Sex#131,Vict Descent#132,Premis Cd#133,Premis Desc#134,Weapon Used Cd#135,Weapon Desc#136,Status#137,Status Desc#138,Crm Cd 1#139,Crm Cd 2#140,Crm Cd 3#141,Crm Cd 4#142,... 4 more fields] csv\n",
      "               :              +- SubqueryAlias c\n",
      "               :                 +- SubqueryAlias census\n",
      "               :                    +- View (`census`, [Zip Code#224,Population#225L,Housing#226L,COMM#227,Coordinates#228,wkt_geometry#86,geometry#93])\n",
      "               :                       +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "               :                          +- Project [Zip Code#224, Population#225L, Housing#226L, COMM#227, Coordinates#228, geojson_to_wkt(Coordinates#228)#85 AS wkt_geometry#86]\n",
      "               :                             +- Project [properties#222.ZCTA10 AS Zip Code#224, properties#222.POP_2010 AS Population#225L, properties#222.HOUSING10 AS Housing#226L, properties#222.COMM AS COMM#227, geometry#221.coordinates AS Coordinates#228]\n",
      "               :                                +- Relation [_corrupt_record#220,geometry#221,properties#222,type#223] json\n",
      "               +- Aggregate [COMM#241], [COMM#241, round(avg(Estimated Median Income#48), 2) AS Median Income#197]\n",
      "                  +- Project [Zip Code#42, Estimated Median Income#48, COMM#241]\n",
      "                     +- Join Inner, (Zip Code#42 = cast(Zip Code#238 as int))\n",
      "                        :- Project [Zip Code#42, Estimated Median Income#48]\n",
      "                        :  +- Project [Zip Code#42, Community#43, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "                        :     +- Relation [Zip Code#42,Community#43,Estimated Median Income#44] csv\n",
      "                        +- Project [Zip Code#238, COMM#241]\n",
      "                           +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, wkt_geometry#86,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "                              +- Project [Zip Code#238, Population#239L, Housing#240L, COMM#241, Coordinates#242, geojson_to_wkt(Coordinates#242)#85 AS wkt_geometry#86]\n",
      "                                 +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.POP_2010 AS Population#239L, properties#236.HOUSING10 AS Housing#240L, properties#236.COMM AS COMM#241, geometry#235.coordinates AS Coordinates#242]\n",
      "                                    +- Relation [_corrupt_record#234,geometry#235,properties#236,type#237] json\n",
      "\n",
      "== Optimized Logical Plan ==\n",
      "Sort [Crime Per Person#249 DESC NULLS LAST], true\n",
      "+- Project [COMM#72, Total Population#208L, Crime Count#217L, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249, Median Income#197, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "   +- Join Inner, (COMM#72 = COMM#241)\n",
      "      :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "      :  +- Join LeftOuter, (COMM#72 = COMM#227)\n",
      "      :     :- Aggregate [COMM#72], [COMM#72, sum(Population#70L) AS Total Population#208L, sum(Housing#71L) AS Total Housing#210L]\n",
      "      :     :  +- Project [properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72]\n",
      "      :     :     +- Filter isnotnull(properties#63.COMM)\n",
      "      :     :        +- Relation [_corrupt_record#61,geometry#62,properties#63,type#64] json\n",
      "      :     +- Aggregate [COMM#227], [COMM#227, count(DR_NO#119) AS Crime Count#217L]\n",
      "      :        +- Project [COMM#227, DR_NO#119]\n",
      "      :           +- Join Inner,  **org.apache.spark.sql.sedona_sql.expressions.ST_Contains**\n",
      "      :              :- Project [DR_NO#119,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "      :              :  +- Filter ((isnotnull(DATE OCC#121) AND (substring(DATE OCC#121, 7, 4) = 2010)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "      :              :     +- Relation [DR_NO#119,Date Rptd#120,DATE OCC#121,TIME OCC#122,AREA #123,AREA NAME#124,Rpt Dist No#125,Part 1-2#126,Crm Cd#127,Crm Cd Desc#128,Mocodes#129,Vict Age#130,Vict Sex#131,Vict Descent#132,Premis Cd#133,Premis Desc#134,Weapon Used Cd#135,Weapon Desc#136,Status#137,Status Desc#138,Crm Cd 1#139,Crm Cd 2#140,Crm Cd 3#141,Crm Cd 4#142,... 4 more fields] csv\n",
      "      :              +- Project [properties#222.COMM AS COMM#227,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "      :                 +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#271]\n",
      "      :                    +- Project [geometry#221, properties#222]\n",
      "      :                       +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**  )\n",
      "      :                          +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#270]\n",
      "      :                             +- Project [geometry#221, properties#222]\n",
      "      :                                +- Filter isnotnull(properties#222.COMM)\n",
      "      :                                   +- Relation [_corrupt_record#220,geometry#221,properties#222,type#223] json\n",
      "      +- Aggregate [COMM#241], [COMM#241, round(avg(Estimated Median Income#48), 2) AS Median Income#197]\n",
      "         +- Project [Estimated Median Income#48, COMM#241]\n",
      "            +- Join Inner, (Zip Code#42 = cast(Zip Code#238 as int))\n",
      "               :- Project [Zip Code#42, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "               :  +- Filter isnotnull(Zip Code#42)\n",
      "               :     +- Relation [Zip Code#42,Community#43,Estimated Median Income#44] csv\n",
      "               +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.COMM AS COMM#241]\n",
      "                  +- Filter (isnotnull(properties#236.ZCTA10) AND isnotnull(properties#236.COMM))\n",
      "                     +- Relation [_corrupt_record#234,geometry#235,properties#236,type#237] json\n",
      "\n",
      "== Physical Plan ==\n",
      "AdaptiveSparkPlan isFinalPlan=false\n",
      "+- Sort [Crime Per Person#249 DESC NULLS LAST], true, 0\n",
      "   +- Exchange rangepartitioning(Crime Per Person#249 DESC NULLS LAST, 1000), ENSURE_REQUIREMENTS, [plan_id=251]\n",
      "      +- Project [COMM#72, Total Population#208L, Crime Count#217L, round((cast(Crime Count#217L as double) / cast(Total Population#208L as double)), 6) AS Crime Per Person#249, Median Income#197, round(((Median Income#197 * cast(Total Housing#210L as double)) / cast(Total Population#208L as double)), 2) AS Income per Person#256]\n",
      "         +- SortMergeJoin [COMM#72], [COMM#241], Inner\n",
      "            :- Project [COMM#72, Total Population#208L, Total Housing#210L, Crime Count#217L]\n",
      "            :  +- SortMergeJoin [COMM#72], [COMM#227], LeftOuter\n",
      "            :     :- Sort [COMM#72 ASC NULLS FIRST], false, 0\n",
      "            :     :  +- HashAggregate(keys=[COMM#72], functions=[sum(Population#70L), sum(Housing#71L)], output=[COMM#72, Total Population#208L, Total Housing#210L], schema specialized)\n",
      "            :     :     +- Exchange hashpartitioning(COMM#72, 1000), ENSURE_REQUIREMENTS, [plan_id=228]\n",
      "            :     :        +- HashAggregate(keys=[COMM#72], functions=[partial_sum(Population#70L), partial_sum(Housing#71L)], output=[COMM#72, sum#273L, sum#275L], schema specialized)\n",
      "            :     :           +- Project [properties#63.POP_2010 AS Population#70L, properties#63.HOUSING10 AS Housing#71L, properties#63.COMM AS COMM#72]\n",
      "            :     :              +- Filter isnotnull(properties#63.COMM)\n",
      "            :     :                 +- FileScan json [properties#63] Batched: false, DataFilters: [isnotnull(properties#63.COMM)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CI...\n",
      "            :     +- Sort [COMM#227 ASC NULLS FIRST], false, 0\n",
      "            :        +- HashAggregate(keys=[COMM#227], functions=[count(DR_NO#119)], output=[COMM#227, Crime Count#217L], schema specialized)\n",
      "            :           +- Exchange hashpartitioning(COMM#227, 1000), ENSURE_REQUIREMENTS, [plan_id=230]\n",
      "            :              +- HashAggregate(keys=[COMM#227], functions=[partial_count(DR_NO#119)], output=[COMM#227, count#277L], schema specialized)\n",
      "            :                 +- Project [COMM#227, DR_NO#119]\n",
      "            :                    +- RangeJoin geometry#179: geometry, geometry#93: geometry, WITHIN\n",
      "            :                       :- Project [DR_NO#119,  **org.apache.spark.sql.sedona_sql.expressions.ST_Point**   AS geometry#179]\n",
      "            :                       :  +- Filter ((isnotnull(DATE OCC#121) AND (substring(DATE OCC#121, 7, 4) = 2010)) AND isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_Point**  ))\n",
      "            :                       :     +- FileScan csv [DR_NO#119,DATE OCC#121,LAT#145,LON#146] Batched: false, DataFilters: [isnotnull(DATE OCC#121), (substring(DATE OCC#121, 7, 4) = 2010), isnotnull( **org.apache.spark.s..., Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_D..., PartitionFilters: [], PushedFilters: [IsNotNull(DATE OCC)], ReadSchema: struct<DR_NO:int,DATE OCC:string,LAT:double,LON:double>\n",
      "            :                       +- Project [properties#222.COMM AS COMM#227,  **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**   AS geometry#93]\n",
      "            :                          +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#271]\n",
      "            :                             +- Project [geometry#221, properties#222]\n",
      "            :                                +- Filter isnotnull( **org.apache.spark.sql.sedona_sql.expressions.ST_GeomFromWKT**  )\n",
      "            :                                   +- BatchEvalPython [geojson_to_wkt(geometry#221.coordinates)#85], [pythonUDF0#270]\n",
      "            :                                      +- Filter isnotnull(properties#222.COMM)\n",
      "            :                                         +- FileScan json [geometry#221,properties#222] Batched: false, DataFilters: [isnotnull(properties#222.COMM)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<geometry:struct<coordinates:array<array<array<string>>>,type:string>,properties:struct<BG1...\n",
      "            +- Sort [COMM#241 ASC NULLS FIRST], false, 0\n",
      "               +- HashAggregate(keys=[COMM#241], functions=[avg(Estimated Median Income#48)], output=[COMM#241, Median Income#197], schema specialized)\n",
      "                  +- Exchange hashpartitioning(COMM#241, 1000), ENSURE_REQUIREMENTS, [plan_id=243]\n",
      "                     +- HashAggregate(keys=[COMM#241], functions=[partial_avg(Estimated Median Income#48)], output=[COMM#241, sum#280, count#281L], schema specialized)\n",
      "                        +- Project [Estimated Median Income#48, COMM#241]\n",
      "                           +- BroadcastHashJoin [Zip Code#42], [cast(Zip Code#238 as int)], Inner, BuildLeft, false\n",
      "                              :- BroadcastExchange HashedRelationBroadcastMode(List(cast(input[0, int, false] as bigint)),false), [plan_id=238]\n",
      "                              :  +- Project [Zip Code#42, cast(regexp_replace(Estimated Median Income#44, [$,], , 1) as float) AS Estimated Median Income#48]\n",
      "                              :     +- Filter isnotnull(Zip Code#42)\n",
      "                              :        +- FileScan csv [Zip Code#42,Estimated Median Income#44] Batched: false, DataFilters: [isnotnull(Zip Code#42)], Format: CSV, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv], PartitionFilters: [], PushedFilters: [IsNotNull(Zip Code)], ReadSchema: struct<Zip Code:int,Estimated Median Income:string>\n",
      "                              +- Project [properties#236.ZCTA10 AS Zip Code#238, properties#236.COMM AS COMM#241]\n",
      "                                 +- Filter (isnotnull(properties#236.ZCTA10) AND isnotnull(properties#236.COMM))\n",
      "                                    +- FileScan json [properties#236] Batched: false, DataFilters: [isnotnull(properties#236.ZCTA10), isnotnull(properties#236.COMM)], Format: JSON, Location: InMemoryFileIndex(1 paths)[s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Block..., PartitionFilters: [], PushedFilters: [], ReadSchema: struct<properties:struct<BG10:string,BG10FIP10:string,BG12:string,CB10:string,CEN_FIP13:string,CI...\n",
      "\n",
      "+------------------+----------------+-----------+----------------+-------------+-----------------+\n",
      "|              Area|Total Population|Crime Count|Crime Per Person|Median Income|Income per Person|\n",
      "+------------------+----------------+-----------+----------------+-------------+-----------------+\n",
      "|            Vernon|             112|         71|        0.633929|     18331.28|          4746.49|\n",
      "|   West Chatsworth|              12|          3|            0.25|      61339.0|         20446.33|\n",
      "|           West LA|             702|        154|        0.219373|     87689.58|          8119.41|\n",
      "|          Downtown|           23647|       4192|        0.177274|     27621.85|         19977.86|\n",
      "|      Little Tokyo|            3386|        465|         0.13733|     23004.07|         14110.88|\n",
      "|Wholesale District|           37156|       4105|         0.11048|     23835.11|          7286.02|\n",
      "|      Leimert Park|           14859|       1475|        0.099266|     36948.59|          16085.9|\n",
      "| Faircrest Heights|            3443|        327|        0.094975|     52901.34|          20834.8|\n",
      "|   Sycamore Square|             635|         60|        0.094488|     61147.88|         30814.68|\n",
      "|         Hollywood|           62412|       5681|        0.091024|      45290.0|         25948.94|\n",
      "|     Vermont Vista|           37550|       3364|        0.089587|     30258.84|          8460.39|\n",
      "|        Exposition|            3238|        287|        0.088635|      38330.0|         12322.89|\n",
      "|Century Palms/Cove|           30692|       2703|        0.088069|     32851.29|          8572.46|\n",
      "|     Baldwin Hills|           28637|       2450|        0.085554|     37297.92|         17391.46|\n",
      "|            Venice|           32625|       2652|        0.081287|     81028.74|         46575.69|\n",
      "|       Rancho Park|            6295|        511|        0.081176|      87283.0|         38740.06|\n",
      "|         Hyde Park|           27943|       2241|        0.080199|     38196.31|         14143.69|\n",
      "| Manchester Square|            8247|        654|        0.079302|      39269.0|         14589.57|\n",
      "|    Vermont Knolls|           16678|       1303|        0.078127|     31295.94|         10440.74|\n",
      "|      Harvard Park|           36447|       2832|        0.077702|     33372.25|          9909.03|\n",
      "+------------------+----------------+-----------+----------------+-------------+-----------------+\n",
      "only showing top 20 rows\n",
      "\n",
      "Execution Time: 53.19 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, regexp_replace, round, avg, expr, udf, count, sum as spark_sum\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "# Initialize Spark session with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query3_LA_Analysis\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register Sedona\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load and preprocess income dataset\n",
    "income_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/LA_income_2015.csv\", header=True, inferSchema=True)\n",
    "income_df = income_df.withColumn(\n",
    "    \"Estimated Median Income\",\n",
    "    regexp_replace(col(\"Estimated Median Income\"), \"[$,]\", \"\").cast(\"float\")\n",
    ")\n",
    "\n",
    "# Load and preprocess census dataset\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "# Convert GeoJSON to WKT for Sedona\n",
    "\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "census_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "census_df = census_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "# Load and preprocess crime dataset\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2010\")\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Perform spatial join using Sedona\n",
    "census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "crime_with_comm_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "# Precompute income by COMM to avoid recomputation\n",
    "income_by_comm = income_df.select(\n",
    "    col(\"Zip Code\"),\n",
    "    col(\"Estimated Median Income\")\n",
    ").join(\n",
    "    census_df.select(\"Zip Code\", \"COMM\"),\n",
    "    on=\"Zip Code\",\n",
    "    how=\"inner\"\n",
    ").groupBy(\"COMM\").agg(\n",
    "    round(avg(\"Estimated Median Income\"), 2).alias(\"Median Income\")\n",
    ")\n",
    "\n",
    "# Calculate the final DataFrame with all required information\n",
    "final_df = census_df.groupBy(\"COMM\").agg(\n",
    "    spark_sum(\"Population\").alias(\"Total Population\"),\n",
    "    spark_sum(\"Housing\").alias(\"Total Housing\")\n",
    ").join(\n",
    "    crime_with_comm_df.groupBy(\"COMM\").agg(\n",
    "        count(\"DR_NO\").alias(\"Crime Count\")\n",
    "    ),\n",
    "    on=\"COMM\",\n",
    "    how=\"left\"\n",
    ").join(\n",
    "    income_by_comm,\n",
    "    on=\"COMM\",\n",
    "    how=\"inner\"\n",
    ").withColumn(\n",
    "    \"Crime Per Person\",\n",
    "    round(col(\"Crime Count\") / col(\"Total Population\"), 6)\n",
    ").withColumn(\n",
    "    \"Income per Person\",\n",
    "    round((col(\"Median Income\") * col(\"Total Housing\")) / col(\"Total Population\"), 2)\n",
    ").select(\n",
    "    col(\"COMM\"),\n",
    "    col(\"Total Population\"),\n",
    "    col(\"Crime Count\"),\n",
    "    col(\"Crime Per Person\"),\n",
    "    col(\"Median Income\"),\n",
    "    col(\"Income per Person\")\n",
    ").orderBy(col(\"Crime Per Person\").desc())\n",
    "\n",
    "final_df.explain(mode=\"extended\")\n",
    "\n",
    "# Save results for query 4\n",
    "query3_df = final_df.select(\n",
    "    col(\"COMM\"),\n",
    "    col(\"Income per Person\")\n",
    ")\n",
    "\n",
    "query3_df.write.mode(\"ignore\").option(\"header\", \"true\").csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/\")\n",
    "\n",
    "# Display final results\n",
    "final_df_area = final_df.withColumnRenamed(\"COMM\", \"Area\")\n",
    "final_df_area.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "# Print execution time\n",
    "print(f\"Execution Time: {end_time - start_time:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c664ef8-441a-4418-af83-1fedac12ecb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2de95ad-7d26-42bd-b082-277144d2c433",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 1core/2GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a4b1db4-5815-4885-ae58-321c699dacb9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victim Counts by Race in Top 3 High-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|               White|      544|\n",
      "|               Other|       73|\n",
      "|Hispanic/Latin/Me...|       60|\n",
      "|             Unknown|       41|\n",
      "|               Black|       37|\n",
      "|         Other Asian|       15|\n",
      "|             Chinese|        1|\n",
      "|American Indian/A...|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "\n",
      "Victim Counts by Race in Bottom 3 Low-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|Hispanic/Latin/Me...|     1494|\n",
      "|               Black|      456|\n",
      "|               Other|       53|\n",
      "|               White|       29|\n",
      "|         Other Asian|        4|\n",
      "|             Unknown|        3|\n",
      "|            Filipino|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "Total execution time: 40.14 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, to_date, year, regexp_replace, count, expr, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "# Create Spark Session with Sedona enabled\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spatial Join - 1 Core, 2GB Memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", 1) \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Register Sedona\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "# Load census.geojson\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "# Convert to wkt with udf\n",
    "\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "\n",
    "wkt_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "wkt_df.select(\"wkt_geometry\")\n",
    "wkt_df = wkt_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "wkt_census_df = wkt_df.select(\"COMM\", \"geometry\")\n",
    "\n",
    "# Load crime.csv\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    "\n",
    "# Select necessary columns and create point geometries\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "# Load income per person from query3\n",
    "income_per_person_df = spark.read.csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/part-00000-5e1158bd-fb96-45fd-a2a9-7bf304269966-c000.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Load race.csv\n",
    "race_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Perform Spatial Join\n",
    "# Register spatial DataFrames as tables\n",
    "wkt_census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "# Perform spatial join using Sedona's ST_Contains\n",
    "crime_with_zip_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO, cr.`Vict Descent`\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "# Aggregate Crimes by Zip Code and Race\n",
    "crime_with_race_df = crime_with_zip_df.join(race_df, \"Vict Descent\", \"inner\")\n",
    "grouped_df = crime_with_race_df.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"*\").alias(\"CrimeCount\"))\n",
    "\n",
    "# Filter Top and Bottom 3 Communities by Income\n",
    "top_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").desc()).select(\"COMM\").limit(3)\n",
    "bottom_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").asc()).select(\"COMM\").limit(3)\n",
    "\n",
    "# Convert top and bottom communities to lists for filtering\n",
    "top_3_comm_list = [row[\"COMM\"] for row in top_3_comm.collect()]\n",
    "bottom_3_comm_list = [row[\"COMM\"] for row in bottom_3_comm.collect()]\n",
    "\n",
    "# Filter crime data for Top and Bottom Communities\n",
    "filtered_crime_top = crime_with_race_df.filter(col(\"COMM\").isin(top_3_comm_list))\n",
    "filtered_crime_bottom = crime_with_race_df.filter(col(\"COMM\").isin(bottom_3_comm_list))\n",
    "\n",
    "\n",
    "# Aggregate Crime Counts by Race for Each Group\n",
    "top_crime_race_df = filtered_crime_top.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "bottom_crime_race_df = filtered_crime_bottom.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "# Display Results\n",
    "print(\"Victim Counts by Race in Top 3 High-Income Communities:\")\n",
    "top_crime_race_df.show()\n",
    "\n",
    "print(\"\\nVictim Counts by Race in Bottom 3 Low-Income Communities:\")\n",
    "bottom_crime_race_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "# Stop Spark Session\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aabf0bd5-d3b1-422c-9bec-69d130f91620",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2cores/4GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "67e098e2-d4e5-4c73-8cb6-d34ed845bde1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3672</td><td>application_1732639283265_3618</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3618/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-181.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3618_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victim Counts by Race in Top 3 High-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|               White|      544|\n",
      "|               Other|       73|\n",
      "|Hispanic/Latin/Me...|       60|\n",
      "|             Unknown|       41|\n",
      "|               Black|       37|\n",
      "|         Other Asian|       15|\n",
      "|             Chinese|        1|\n",
      "|American Indian/A...|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "\n",
      "Victim Counts by Race in Bottom 3 Low-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|Hispanic/Latin/Me...|     1494|\n",
      "|               Black|      456|\n",
      "|               Other|       53|\n",
      "|               White|       29|\n",
      "|         Other Asian|        4|\n",
      "|             Unknown|        3|\n",
      "|            Filipino|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "Total execution time: 34.99 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, to_date, year, regexp_replace, count, expr, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spatial Join - 1 Core, 2GB Memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", 2) \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "\n",
    "wkt_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "wkt_df.select(\"wkt_geometry\")\n",
    "wkt_df = wkt_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "wkt_census_df = wkt_df.select(\"COMM\", \"geometry\")\n",
    "\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    "\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "income_per_person_df = spark.read.csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/part-00000-5e1158bd-fb96-45fd-a2a9-7bf304269966-c000.csv\", header=True, inferSchema=True)\n",
    "\n",
    "race_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "wkt_census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "crime_with_zip_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO, cr.`Vict Descent`\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "crime_with_race_df = crime_with_zip_df.join(race_df, \"Vict Descent\", \"inner\")\n",
    "grouped_df = crime_with_race_df.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"*\").alias(\"CrimeCount\"))\n",
    "\n",
    "top_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").desc()).select(\"COMM\").limit(3)\n",
    "bottom_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").asc()).select(\"COMM\").limit(3)\n",
    "\n",
    "top_3_comm_list = [row[\"COMM\"] for row in top_3_comm.collect()]\n",
    "bottom_3_comm_list = [row[\"COMM\"] for row in bottom_3_comm.collect()]\n",
    "\n",
    "filtered_crime_top = crime_with_race_df.filter(col(\"COMM\").isin(top_3_comm_list))\n",
    "filtered_crime_bottom = crime_with_race_df.filter(col(\"COMM\").isin(bottom_3_comm_list))\n",
    "\n",
    "top_crime_race_df = filtered_crime_top.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "bottom_crime_race_df = filtered_crime_bottom.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "print(\"Victim Counts by Race in Top 3 High-Income Communities:\")\n",
    "top_crime_race_df.show()\n",
    "\n",
    "print(\"\\nVictim Counts by Race in Bottom 3 Low-Income Communities:\")\n",
    "bottom_crime_race_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d75f18d5-ad07-4f96-bbb3-e5a6b2036a99",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4cores/8GB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8fbf6cf-bc53-487a-84b0-711498a71efe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>3674</td><td>application_1732639283265_3620</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-192-168-1-36.eu-central-1.compute.internal:20888/proxy/application_1732639283265_3620/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-192-168-1-166.eu-central-1.compute.internal:8042/node/containerlogs/container_1732639283265_3620_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Victim Counts by Race in Top 3 High-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|               White|      544|\n",
      "|               Other|       73|\n",
      "|Hispanic/Latin/Me...|       60|\n",
      "|             Unknown|       41|\n",
      "|               Black|       37|\n",
      "|         Other Asian|       15|\n",
      "|             Chinese|        1|\n",
      "|American Indian/A...|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "\n",
      "Victim Counts by Race in Bottom 3 Low-Income Communities:\n",
      "+--------------------+---------+\n",
      "|   Vict Descent Full|VictCount|\n",
      "+--------------------+---------+\n",
      "|Hispanic/Latin/Me...|     1494|\n",
      "|               Black|      456|\n",
      "|               Other|       53|\n",
      "|               White|       29|\n",
      "|         Other Asian|        4|\n",
      "|             Unknown|        3|\n",
      "|            Filipino|        1|\n",
      "+--------------------+---------+\n",
      "\n",
      "Total execution time: 29.24 seconds"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from sedona.register import SedonaRegistrator\n",
    "from sedona.sql.types import GeometryType\n",
    "from pyspark.sql.functions import col, to_date, year, regexp_replace, count, expr, udf\n",
    "from pyspark.sql.types import StringType\n",
    "from shapely.geometry import shape\n",
    "from shapely.geometry.polygon import Polygon\n",
    "from shapely import wkt\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spatial Join - 1 Core, 2GB Memory\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", 4) \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.kryo.registrator\", \"org.apache.sedona.core.serde.SedonaKryoRegistrator\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "census_df = spark.read.json(\"s3://initial-notebook-data-bucket-dblab-905418150721/2010_Census_Blocks.geojson\")\n",
    "\n",
    "census_df = census_df.select(\n",
    "    col(\"properties.ZCTA10\").alias(\"Zip Code\"),\n",
    "    col(\"properties.POP_2010\").alias(\"Population\"),\n",
    "    col(\"properties.HOUSING10\").alias(\"Housing\"),\n",
    "    col(\"properties.COMM\").alias(\"COMM\"),\n",
    "    col(\"geometry.coordinates\").alias(\"Coordinates\")\n",
    ")\n",
    "\n",
    "def geojson_to_wkt(coords):\n",
    "    if coords is None:\n",
    "        return None\n",
    "    try:\n",
    "        geom = shape({\"type\": \"Polygon\", \"coordinates\": coords})\n",
    "        return wkt.dumps(geom)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing coordinates: {coords}, Error: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "geojson_to_wkt_udf = udf(geojson_to_wkt, StringType())\n",
    "\n",
    "wkt_df = census_df.withColumn(\"wkt_geometry\", geojson_to_wkt_udf(census_df[\"Coordinates\"]))\n",
    "wkt_df.select(\"wkt_geometry\")\n",
    "wkt_df = wkt_df.withColumn(\"geometry\", expr(\"ST_GeomFromWKT(wkt_geometry)\"))\n",
    "\n",
    "wkt_census_df = wkt_df.select(\"COMM\", \"geometry\")\n",
    "\n",
    "crime_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\", header=True, inferSchema=True)\n",
    "crime_df = crime_df.filter(col(\"DATE OCC\").substr(7, 4) == \"2015\")\n",
    "\n",
    "crime_df = crime_df.select(\n",
    "    col(\"DR_NO\"),\n",
    "    col(\"Vict Descent\"),\n",
    "    col(\"LAT\"),\n",
    "    col(\"LON\")\n",
    ").withColumn(\"geometry\", expr(\"ST_Point(LON, LAT)\"))\n",
    "\n",
    "income_per_person_df = spark.read.csv(\"s3://groups-bucket-dblab-905418150721/group28/query3/part-00000-5e1158bd-fb96-45fd-a2a9-7bf304269966-c000.csv\", header=True, inferSchema=True)\n",
    "\n",
    "race_df = spark.read.csv(\"s3://initial-notebook-data-bucket-dblab-905418150721/RE_codes.csv\", header=True, inferSchema=True)\n",
    "\n",
    "wkt_census_df.createOrReplaceTempView(\"census\")\n",
    "crime_df.createOrReplaceTempView(\"crime\")\n",
    "\n",
    "crime_with_zip_df = spark.sql(\"\"\"\n",
    "    SELECT c.COMM, cr.DR_NO, cr.`Vict Descent`\n",
    "    FROM crime cr\n",
    "    JOIN census c\n",
    "    ON ST_Contains(c.geometry, cr.geometry)\n",
    "\"\"\")\n",
    "\n",
    "crime_with_race_df = crime_with_zip_df.join(race_df, \"Vict Descent\", \"inner\")\n",
    "grouped_df = crime_with_race_df.groupBy(\"COMM\", \"Vict Descent Full\").agg(count(\"*\").alias(\"CrimeCount\"))\n",
    "\n",
    "top_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").desc()).select(\"COMM\").limit(3)\n",
    "bottom_3_comm = income_per_person_df.orderBy(col(\"Income Per Person\").asc()).select(\"COMM\").limit(3)\n",
    "\n",
    "top_3_comm_list = [row[\"COMM\"] for row in top_3_comm.collect()]\n",
    "bottom_3_comm_list = [row[\"COMM\"] for row in bottom_3_comm.collect()]\n",
    "\n",
    "filtered_crime_top = crime_with_race_df.filter(col(\"COMM\").isin(top_3_comm_list))\n",
    "filtered_crime_bottom = crime_with_race_df.filter(col(\"COMM\").isin(bottom_3_comm_list))\n",
    "\n",
    "top_crime_race_df = filtered_crime_top.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "bottom_crime_race_df = filtered_crime_bottom.groupBy(\"Vict Descent Full\").agg(\n",
    "    count(\"*\").alias(\"VictCount\")\n",
    ").orderBy(col(\"VictCount\").desc())\n",
    "\n",
    "print(\"Victim Counts by Race in Top 3 High-Income Communities:\")\n",
    "top_crime_race_df.show()\n",
    "\n",
    "print(\"\\nVictim Counts by Race in Bottom 3 Low-Income Communities:\")\n",
    "bottom_crime_race_df.show()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8647757c-bcb4-43f3-8e25-d50d89bfe33c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Query 5\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ce7840-4027-44aa-90c4-1ae72eac9ee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, sum, when, row_number, expr\n",
    "from pyspark.sql.window import Window\n",
    "import time\n",
    "\n",
    "# Start timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Start Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Query2 DataFrame API\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load datasets\n",
    "crime_data_2010_2019 = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2010_to_2019_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "crime_data_2020_present = spark.read.csv(\n",
    "    \"s3://initial-notebook-data-bucket-dblab-905418150721/CrimeData/Crime_Data_from_2020_to_Present_20241101.csv\",\n",
    "    header=True, inferSchema=True\n",
    ")\n",
    "\n",
    "# Combine both datasets\n",
    "crime_data = crime_data_2010_2019.union(crime_data_2020_present)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11819c4e-18f4-4f90-9d30-2dab142d1e5c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Columns in the CSV file:\n",
      "['X', 'Y', 'FID', 'DIVISION', 'LOCATION', 'PREC']\n",
      "Schema of the CSV file:\n",
      "root\n",
      " |-- X: double (nullable = true)\n",
      " |-- Y: double (nullable = true)\n",
      " |-- FID: integer (nullable = true)\n",
      " |-- DIVISION: string (nullable = true)\n",
      " |-- LOCATION: string (nullable = true)\n",
      " |-- PREC: integer (nullable = true)"
     ]
    }
   ],
   "source": [
    "# Load the CSV file\n",
    "csv_file_path = \"s3://initial-notebook-data-bucket-dblab-905418150721/LA_Police_Stations.csv\"\n",
    "df = spark.read.csv(csv_file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Inspect columns\n",
    "print(\"Columns in the CSV file:\")\n",
    "print(df.columns)\n",
    "\n",
    "# Inspect schema\n",
    "print(\"Schema of the CSV file:\")\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aea1b0-e9b7-48c0-8d73-54240f97213c",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 2 executors × 4 cores/8GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ff827f61-6fda-4e8e-8aa2-b134061a0acf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.02043779072548565 |\n",
      "|VAN NUYS        |211457          |0.028653154590629136|\n",
      "|WILSHIRE        |198150          |0.026312166557481587|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.01729162112331338 |\n",
      "|NORTH HOLLYWOOD |171159          |0.026115214222567722|\n",
      "|77TH STREET     |167323          |0.016584871496068188|\n",
      "|PACIFIC         |157468          |0.037495777088312074|\n",
      "|CENTRAL         |154474          |0.009868086849235298|\n",
      "|SOUTHEAST       |151999          |0.024150127195506455|\n",
      "|RAMPART         |149675          |0.014730484635455721|\n",
      "|TOPANGA         |147167          |0.03243890335156792 |\n",
      "|WEST VALLEY     |130933          |0.02897360719640746 |\n",
      "|HARBOR          |130206          |3.2997622866934675  |\n",
      "|FOOTHILL        |122515          |0.04125740608010438 |\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859795|\n",
      "|HOLLENBECK      |119329          |0.026407445235885316|\n",
      "|NEWTON          |109078          |0.015890866822603905|\n",
      "|MISSION         |109009          |0.035032007153604966|\n",
      "|NORTHEAST       |105687          |0.03907902069344001 |\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 56.37 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with Sedona\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.jars.packages\", \"org.apache.sedona:sedona-sql-3.0_2.12:1.6.1\") \\\n",
    "    .config(\"spark.executor.instances\", \"2\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.executor.memory\", \"8g\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e597d39-633e-4996-ba23-69e99542d912",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 4 executors × 2 cores/4GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "901e1710-1a32-4790-bade-85a902d818e3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.020437790725485655|\n",
      "|VAN NUYS        |211457          |0.02865315459062913 |\n",
      "|WILSHIRE        |198150          |0.026312166557481583|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.01729162112331337 |\n",
      "|NORTH HOLLYWOOD |171159          |0.026115214222567722|\n",
      "|77TH STREET     |167323          |0.016584871496068194|\n",
      "|PACIFIC         |157468          |0.03749577708831207 |\n",
      "|CENTRAL         |154474          |0.0098680868492353  |\n",
      "|SOUTHEAST       |151999          |0.024150127195506462|\n",
      "|RAMPART         |149675          |0.014730484635455718|\n",
      "|TOPANGA         |147167          |0.03243890335156792 |\n",
      "|WEST VALLEY     |130933          |0.028973607196407465|\n",
      "|HARBOR          |130206          |3.2997622866934675  |\n",
      "|FOOTHILL        |122515          |0.041257406080104385|\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859802|\n",
      "|HOLLENBECK      |119329          |0.02640744523588532 |\n",
      "|NEWTON          |109078          |0.0158908668226039  |\n",
      "|MISSION         |109009          |0.03503200715360496 |\n",
      "|NORTHEAST       |105687          |0.039079020693440006|\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 46.34 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with specified resources\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.executor.instances\", \"4\") \\\n",
    "    .config(\"spark.executor.cores\", \"2\") \\\n",
    "    .config(\"spark.executor.memory\", \"4g\") \\\n",
    "    .config(\"spark.driver.memory\", \"4g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49f9eb7b-22fa-45fb-a9cf-c1f1aa0635b2",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### 8 executors × 1 core/2 GB memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "61735c16-b7d8-4e55-8d3c-7422fdc72acd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+----------------+--------------------+\n",
      "|DIVISION        |number_of_crimes|average_distance    |\n",
      "+----------------+----------------+--------------------+\n",
      "|HOLLYWOOD       |213080          |0.02043779072548566 |\n",
      "|VAN NUYS        |211457          |0.028653154590629126|\n",
      "|WILSHIRE        |198150          |0.026312166557481583|\n",
      "|SOUTHWEST       |186742          |0.021577001184243143|\n",
      "|OLYMPIC         |180463          |0.017291621123313373|\n",
      "|NORTH HOLLYWOOD |171159          |0.02611521422256772 |\n",
      "|77TH STREET     |167323          |0.016584871496068188|\n",
      "|PACIFIC         |157468          |0.037495777088312074|\n",
      "|CENTRAL         |154474          |0.009868086849235298|\n",
      "|SOUTHEAST       |151999          |0.02415012719550645 |\n",
      "|RAMPART         |149675          |0.014730484635455718|\n",
      "|TOPANGA         |147167          |0.03243890335156791 |\n",
      "|WEST VALLEY     |130933          |0.02897360719640747 |\n",
      "|HARBOR          |130206          |3.299762286693468   |\n",
      "|FOOTHILL        |122515          |0.041257406080104385|\n",
      "|WEST LOS ANGELES|121074          |0.029842606564859795|\n",
      "|HOLLENBECK      |119329          |0.026407445235885323|\n",
      "|NEWTON          |109078          |0.0158908668226039  |\n",
      "|MISSION         |109009          |0.03503200715360496 |\n",
      "|NORTHEAST       |105687          |0.039079020693440006|\n",
      "|DEVONSHIRE      |76349           |0.02762167654642529 |\n",
      "+----------------+----------------+--------------------+\n",
      "\n",
      "Total execution time: 16.25 seconds"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from sedona.register.geo_registrator import SedonaRegistrator\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import expr, col, mean, count, min, first\n",
    "\n",
    "# Start the timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Initialize Spark Session with specified resources\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"GeospatialQuery\") \\\n",
    "    .config(\"spark.executor.instances\", \"8\") \\\n",
    "    .config(\"spark.executor.cores\", \"1\") \\\n",
    "    .config(\"spark.executor.memory\", \"2g\") \\\n",
    "    .config(\"spark.driver.memory\", \"2g\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "SedonaRegistrator.registerAll(spark)\n",
    "\n",
    "# Create Spatial DataFrames\n",
    "# Police stations use X (longitude) and Y (latitude)\n",
    "df = df.withColumn(\"station_geometry\", expr(\"ST_Point(cast(X as Decimal(24, 20)), cast(Y as Decimal(24, 20)))\"))\n",
    "\n",
    "# Crimes use LAT (latitude) and LON (longitude)\n",
    "crime_data = crime_data.withColumn(\"crime_geometry\", expr(\"ST_Point(cast(LON as Decimal(24, 20)), cast(LAT as Decimal(24, 20)))\"))\n",
    "\n",
    "# Perform Spatial Join to calculate distances from all crimes to all police stations\n",
    "df_broadcast = df.select(\"DIVISION\", \"station_geometry\").cache()\n",
    "\n",
    "# Calculate distances\n",
    "distances = crime_data.crossJoin(df_broadcast) \\\n",
    "    .withColumn(\"distance\", expr(\"ST_Distance(crime_geometry, station_geometry)\"))\n",
    "\n",
    "# Assign each crime to its closest division\n",
    "closest_crimes = distances.withColumn(\"rank\", expr(\"ROW_NUMBER() OVER (PARTITION BY DR_NO ORDER BY distance ASC)\")) \\\n",
    "    .filter(col(\"rank\") == 1) \\\n",
    "    .select(\"DR_NO\", \"distance\", \"DIVISION\")\n",
    "\n",
    "# Aggregate by division to calculate the number of crimes and average distance\n",
    "result = closest_crimes.groupBy(\"DIVISION\").agg(\n",
    "    count(\"*\").alias(\"number_of_crimes\"),\n",
    "    mean(\"distance\").alias(\"average_distance\")\n",
    ").orderBy(col(\"number_of_crimes\").desc())\n",
    "\n",
    "# Show all rows of the result\n",
    "result.show(result.count(), truncate=False)\n",
    "\n",
    "# End the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate and display the runtime\n",
    "runtime = end_time - start_time\n",
    "print(f\"Total execution time: {runtime:.2f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e62bf6a-f6fa-4bf8-875c-ffac054a79de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Sparkmagic (PySpark)",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
